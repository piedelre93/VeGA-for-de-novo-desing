{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7463bb2-9dea-4c42-9627-72fd3fe0a5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/grad/Desktop/pietro/denovo/new/attachments_3/2/diff_scaffold.h5\"\n",
    "char2idx_path = \"/home/grad/Desktop/pietro/denovo/new/attachments_3/2/char2idx.pkl\"\n",
    "idx2char_path = \"/home/grad/Desktop/pietro/denovo/new/attachments_3/2/idx2char.pkl\"\n",
    "vocab_path    = \"/home/grad/Desktop/pietro/denovo/new/attachments_3/2/vocab.json\"\n",
    "model_out_path = \"/home/grad/Desktop/pietro/denovo/new/risultati/fine/gba/model_final_fxar_pp.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24acb728-5152-40b3-a4ac-a9e046bf933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURAZIONE ---\n",
    "class AdvancedConfig:\n",
    "    SMILES_FILE = '/home/grad/Desktop/pietro/denovo/s4-for-de-novo-drug-design/datasets/fxar/fine.txt'   # File per il fine-tuning\n",
    "    BATCH_SIZE = 10\n",
    "    EPOCHS = 50                  # Numero di epoche per il fine-tuning\n",
    "    EMBED_DIM = 94             # Deve corrispondere a quello del modello pre-addestrato\n",
    "    TRANSFORMER_HEADS = 4\n",
    "    TRANSFORMER_LAYERS = 4\n",
    "    FF_DIM = 300\n",
    "    VALID_RATIO = 0.1            # Percentuale di validation split\n",
    "    TEMPERATURE = 1\n",
    "    TEMPERATURE_DECAY = 0.97\n",
    "    GEN_NUM = 5\n",
    "    WARMUP_EPOCHS = 10\n",
    "    MAX_RANDOMIZATIONS = 10\n",
    "    MAX_LENGTH = 94              # Lunghezza massima (potrà essere aggiornata in seguito)\n",
    "    PRINT_EVERY = 5              # Non usato più per la generazione di SMILES\n",
    "    DROPOUT_RATE = 0.15\n",
    "    GRADIENT_CLIP = 1.0\n",
    "    L2_REG = 1e-5\n",
    "    LOSS_STABILITY_THRESHOLD = 0.01\n",
    "    CURRICULUM_START_COMPLEXITY = 0\n",
    "    CURRICULUM_COMPLEXITY_STEP = 1\n",
    "    AUGMENT_PROB = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c7ca76a-d50d-4a39-8ee8-58113b39091d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 09:48:17.757039: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-11 09:48:17.771081: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749628097.788029 3718496 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749628097.793231 3718496 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749628097.805974 3718496 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749628097.805999 3718496 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749628097.806000 3718496 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749628097.806002 3718496 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-11 09:48:17.810449: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Script di fine-tuning aggiornato:\n",
    "  - Suddivide il dataset in train e validation.\n",
    "  - Valuta la loss sul validation set ad ogni epoca.\n",
    "  - Non genera SMILES ogni 5 epoche.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import logging\n",
    "import pickle\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Embedding, Input, LayerNormalization, MultiHeadAttention, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MolFromSmiles, MolToSmiles\n",
    "from threading import Lock\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configurazione del logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "\n",
    "\n",
    "config = AdvancedConfig()\n",
    "\n",
    "# --- FUNZIONI DI PREPROCESSING ---\n",
    "def validate_and_fix_smiles(smiles: str) -> str:\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        try:\n",
    "            Chem.Kekulize(mol, clearAromaticFlags=True)\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Kekulization error in SMILES {smiles}: {e}\")\n",
    "            return None\n",
    "        return Chem.MolToSmiles(mol, canonical=True, isomericSmiles=False)\n",
    "    except Exception as e:\n",
    "        logger.debug(f\"Parsing/sanitization error in SMILES {smiles}: {e}\")\n",
    "        return None\n",
    "\n",
    "def robust_tokenize(smiles: str) -> list:\n",
    "    pattern = (\n",
    "        r\"(\\[[^\\[\\]]{1,6}\\]|\"                 # atomi in parentesi quadre\n",
    "        r\"Br|Cl|Si|Na|Mg|Mn|Ca|Fe|Zn|Se|Li|K|Al|B|\"  # elementi multi-char\n",
    "        r\"R[0-9]|r[0-9]|a[0-9]|\"             # ring labels\n",
    "        r\"[A-Za-z0-9@+\\-\\\\\\/\\(\\)=#\\$\\.\\%,])\"  # singoli caratteri, incluso '%'\n",
    "    )\n",
    "    tokens = re.findall(pattern, smiles)\n",
    "    stack = []\n",
    "    for t in tokens:\n",
    "        if t.startswith('['):\n",
    "            stack.append(t)\n",
    "        if t.endswith(']') and not stack:\n",
    "            return []\n",
    "        if t.endswith(']'):\n",
    "            stack.pop()\n",
    "    try:\n",
    "        if not stack and Chem.MolFromSmiles(''.join(tokens)):\n",
    "            return tokens\n",
    "    except Exception as e:\n",
    "        logger.debug(f\"Tokenization error: {e}\")\n",
    "        return []\n",
    "    return tokens\n",
    "\n",
    "def process_dataset(data: list) -> tuple:\n",
    "    processed = []\n",
    "    all_tokens = set()\n",
    "    for s in data:\n",
    "        fixed = validate_and_fix_smiles(s)\n",
    "        if not fixed:\n",
    "            continue\n",
    "        tokens = robust_tokenize(fixed)\n",
    "        if tokens and 3 <= len(tokens) <= config.MAX_LENGTH - 2:\n",
    "            processed.append(tokens)\n",
    "            all_tokens.update(tokens)\n",
    "    vocab = ['<PAD>', '<START>', '<END>'] + sorted(all_tokens)\n",
    "    lengths = [len(t) for t in processed]\n",
    "    max_len = min(int(np.percentile(lengths, 99)) + 2, config.MAX_LENGTH) if processed else config.MAX_LENGTH\n",
    "    logger.info(f\"Processed SMILES: {len(processed)}/{len(data)}\")\n",
    "    logger.info(f\"Unique tokens: {len(all_tokens)}\")\n",
    "    logger.info(f\"Max length: {max_len}\")\n",
    "    return processed, vocab, max_len\n",
    "\n",
    "def randomize_smiles(smiles: str, num_versions: int = 3) -> list:\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if not mol:\n",
    "        return []\n",
    "    randomized = []\n",
    "    for _ in range(num_versions):\n",
    "        try:\n",
    "            new_smiles = Chem.MolToSmiles(mol, doRandom=True, canonical=False)\n",
    "            if new_smiles:\n",
    "                randomized.append(new_smiles)\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Randomization error: {e}\")\n",
    "            continue\n",
    "    return randomized\n",
    "\n",
    "def compute_complexity_from_tokens(tokens: list) -> int:\n",
    "    smiles = ''.join(tokens)\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if not mol:\n",
    "            return float('inf')\n",
    "        num_rings = Chem.GetSSSR(mol)\n",
    "        num_branches = smiles.count('(')\n",
    "        return num_rings + num_branches\n",
    "    except Exception:\n",
    "        return float('inf')\n",
    "\n",
    "# --- Componenti del Modello ---\n",
    "class DynamicPositionalEncoding(Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "    def build(self, input_shape):\n",
    "        max_seq_len = input_shape[1]\n",
    "        pos = np.arange(max_seq_len)[:, np.newaxis]\n",
    "        i = np.arange(self.embed_dim)[np.newaxis, :]\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(self.embed_dim))\n",
    "        angle_rads = pos * angle_rates\n",
    "        angle_rads[:, 0::2] = tf.math.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = tf.math.cos(angle_rads[:, 1::2])\n",
    "        self.pos_encoding = tf.cast(angle_rads[np.newaxis, ...], dtype=tf.float32)\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        base_config.update({\"embed_dim\": self.embed_dim})\n",
    "        return base_config\n",
    "\n",
    "class ImprovedTransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim, rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.rate = rate\n",
    "        self.mha = MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embed_dim,\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(config.L2_REG),\n",
    "            dropout=rate\n",
    "        )\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ffn_dim, activation=\"gelu\", kernel_regularizer=tf.keras.regularizers.l2(config.L2_REG)),\n",
    "            Dense(embed_dim, kernel_regularizer=tf.keras.regularizers.l2(config.L2_REG))\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        causal_mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        attn_output = self.mha(inputs, inputs, attention_mask=causal_mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        base_config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"ffn_dim\": self.ffn_dim,\n",
    "            \"rate\": self.rate\n",
    "        })\n",
    "        return base_config\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, embed_dim, warmup_steps=10000):\n",
    "        super().__init__()\n",
    "        self.embed_dim = tf.cast(embed_dim, tf.float32)\n",
    "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32) + 1e-9\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.embed_dim) * tf.math.minimum(arg1, arg2)\n",
    "    def get_config(self):\n",
    "        return {\"embed_dim\": self.embed_dim.numpy(), \"warmup_steps\": self.warmup_steps.numpy()}\n",
    "\n",
    "def build_improved_model(vocab_size: int) -> Model:\n",
    "    inputs = Input(shape=(config.MAX_LENGTH,))\n",
    "    x = Embedding(vocab_size, config.EMBED_DIM, mask_zero=True)(inputs)\n",
    "    x = DynamicPositionalEncoding(config.EMBED_DIM)(x)\n",
    "    x = Dropout(config.DROPOUT_RATE)(x)\n",
    "    for _ in range(config.TRANSFORMER_LAYERS):\n",
    "        x = ImprovedTransformerBlock(config.EMBED_DIM, config.TRANSFORMER_HEADS, config.FF_DIM, rate=config.DROPOUT_RATE)(x)\n",
    "    outputs = Dense(vocab_size)(x)\n",
    "    def smoothed_loss(y_true, y_pred):\n",
    "        # cast a interi perché tf.nn.sparse_* vuole etichette int32 o int64\n",
    "        y_true_int = tf.cast(y_true, tf.int32)\n",
    "        mask = tf.cast(tf.math.not_equal(y_true, 0), tf.float32)\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true_int, logits=y_pred)\n",
    "        return tf.reduce_sum(loss * mask) / (tf.reduce_sum(mask) + 1e-9)\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=CustomSchedule(config.EMBED_DIM),\n",
    "        clipnorm=config.GRADIENT_CLIP\n",
    "    )\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=optimizer, loss=smoothed_loss)\n",
    "    return model\n",
    "\n",
    "# --- GENERATORI ---\n",
    "class ThreadSafeIterator:\n",
    "    def __init__(self, iterator):\n",
    "        self.iterator = iterator\n",
    "        self.lock = Lock()\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        with self.lock:\n",
    "            return next(self.iterator)\n",
    "\n",
    "def threadsafe_generator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        return ThreadSafeIterator(func(*args, **kwargs))\n",
    "    return wrapper\n",
    "\n",
    "class CurriculumSmilesGenerator:\n",
    "    \"\"\"\n",
    "    Generatore per il training che utilizza curriculum learning e data augmentation.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenized_smiles: list, vocab: list):\n",
    "        self.char2idx = {c: i for i, c in enumerate(vocab)}\n",
    "        self.idx2char = {i: c for c, i in self.char2idx.items()}\n",
    "        self.original_data = []\n",
    "        for tokens in tokenized_smiles:\n",
    "            comp = compute_complexity_from_tokens(tokens)\n",
    "            fixed = validate_and_fix_smiles(''.join(tokens))\n",
    "            if fixed is None:\n",
    "                continue\n",
    "            self.original_data.append((tokens, comp))\n",
    "        if self.original_data:\n",
    "            valid_comps = [comp for _, comp in self.original_data if comp != float('inf')]\n",
    "            self.max_complexity = max(valid_comps) if valid_comps else 0\n",
    "        else:\n",
    "            self.max_complexity = 0\n",
    "        self.current_complexity = config.CURRICULUM_START_COMPLEXITY\n",
    "        self.available_data = self._filter_data()\n",
    "        self.train_smiles = {''.join(tokens) for tokens, _ in self.original_data}\n",
    "        self.lock = Lock()\n",
    "    \n",
    "    def _filter_data(self):\n",
    "        filtered = [tokens for tokens, comp in self.original_data if comp <= self.current_complexity]\n",
    "        return filtered or [tokens for tokens, comp in self.original_data]\n",
    "    \n",
    "    def update_complexity(self, epoch: int, loss_diff: float = None):\n",
    "        with self.lock:\n",
    "            if loss_diff is not None and loss_diff < config.LOSS_STABILITY_THRESHOLD:\n",
    "                self.current_complexity = min(self.current_complexity + config.CURRICULUM_COMPLEXITY_STEP, self.max_complexity)\n",
    "            else:\n",
    "                if epoch <= config.WARMUP_EPOCHS:\n",
    "                    increment = int((self.max_complexity - config.CURRICULUM_START_COMPLEXITY) * (epoch / config.WARMUP_EPOCHS))\n",
    "                    self.current_complexity = config.CURRICULUM_START_COMPLEXITY + increment\n",
    "                else:\n",
    "                    self.current_complexity = self.max_complexity\n",
    "            self.available_data = self._filter_data()\n",
    "            if not self.available_data:\n",
    "                self.available_data = [tokens for tokens, comp in self.original_data]\n",
    "                logger.warning(\"Reset available_data to original\")\n",
    "    \n",
    "    @threadsafe_generator\n",
    "    def __call__(self):\n",
    "        while True:\n",
    "            inputs = np.full((config.BATCH_SIZE, config.MAX_LENGTH), self.char2idx['<PAD>'], dtype=np.int32)\n",
    "            targets = np.full_like(inputs, self.char2idx['<PAD>'])\n",
    "            for i in range(config.BATCH_SIZE):\n",
    "                with self.lock:\n",
    "                    try:\n",
    "                        tokens = random.choice(self.available_data)\n",
    "                    except IndexError:\n",
    "                        self.available_data = [tokens for tokens, comp in self.original_data]\n",
    "                        tokens = random.choice(self.available_data)\n",
    "                if random.random() < config.AUGMENT_PROB:\n",
    "                    try:\n",
    "                        augmented = randomize_smiles(''.join(tokens))\n",
    "                        if augmented:\n",
    "                            new_tokens = robust_tokenize(random.choice(augmented)) or tokens\n",
    "                            tokens = new_tokens\n",
    "                    except Exception as e:\n",
    "                        logger.debug(f\"Augmentation error: {e}\")\n",
    "                seq = ['<START>'] + tokens + ['<END>']\n",
    "                padded = (seq + ['<PAD>'] * config.MAX_LENGTH)[:config.MAX_LENGTH]\n",
    "                inputs[i] = [self.char2idx.get(t, self.char2idx['<PAD>']) for t in padded]\n",
    "                targets[i, :-1] = inputs[i][1:]\n",
    "                targets[i, -1] = self.char2idx['<PAD>']\n",
    "            yield inputs, targets\n",
    "\n",
    "    def get_dataset(self):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            self.__call__,\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=(config.BATCH_SIZE, config.MAX_LENGTH), dtype=tf.int32),\n",
    "                tf.TensorSpec(shape=(config.BATCH_SIZE, config.MAX_LENGTH), dtype=tf.int32)\n",
    "            )\n",
    "        ).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Per la validazione creiamo un generatore semplice che non applichi data augmentation\n",
    "def simple_smiles_generator(tokenized_smiles: list, vocab: list):\n",
    "    char2idx = {c: i for i, c in enumerate(vocab)}\n",
    "    while True:\n",
    "        inputs = np.full((config.BATCH_SIZE, config.MAX_LENGTH), char2idx['<PAD>'], dtype=np.int32)\n",
    "        targets = np.full_like(inputs, char2idx['<PAD>'])\n",
    "        # Se il validation set è più piccolo, si cicla ripetutamente\n",
    "        chosen = random.choices(tokenized_smiles, k=config.BATCH_SIZE)\n",
    "        for i, tokens in enumerate(chosen):\n",
    "            seq = ['<START>'] + tokens + ['<END>']\n",
    "            padded = (seq + ['<PAD>'] * config.MAX_LENGTH)[:config.MAX_LENGTH]\n",
    "            inputs[i] = [char2idx.get(t, char2idx['<PAD>']) for t in padded]\n",
    "            targets[i, :-1] = inputs[i][1:]\n",
    "            targets[i, -1] = char2idx['<PAD>']\n",
    "        yield inputs, targets\n",
    "\n",
    "def get_simple_dataset(tokenized_smiles: list, vocab: list):\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        lambda: simple_smiles_generator(tokenized_smiles, vocab),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(config.BATCH_SIZE, config.MAX_LENGTH), dtype=tf.int32),\n",
    "            tf.TensorSpec(shape=(config.BATCH_SIZE, config.MAX_LENGTH), dtype=tf.int32)\n",
    "        )\n",
    "    ).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# --- Callback per il monitoraggio ---\n",
    "class CustomTensorBoard(tf.keras.callbacks.TensorBoard):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        lr = self.model.optimizer.learning_rate\n",
    "        if isinstance(lr, tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "            logs['lr'] = lr(epoch).numpy()\n",
    "        else:\n",
    "            logs['lr'] = lr.numpy()\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "\n",
    "class EnhancedTrainingMonitor(Callback):\n",
    "    \"\"\"\n",
    "    Callback di monitoraggio che ora si limita a loggare i valori di loss\n",
    "    (non genera SMILES a ogni PRINT_EVERY epoche).\n",
    "    \"\"\"\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logger.info(f\"Epoca {epoch+1}: Loss Training = {logs.get('loss', 'N/A'):.4f}, Loss Validation = {logs.get('val_loss', 'N/A'):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b577673-010b-4878-887e-c8e62a537b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FINE-TUNING SCRIPT ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Carica il vocabolario e le mappature salvate\n",
    "    logger.info(\"Loading vocabulary and saved mappings...\")\n",
    "    with open(char2idx_path, 'rb') as f:\n",
    "        char2idx = pickle.load(f)\n",
    "    with open(idx2char_path, 'rb') as f:\n",
    "        idx2char = pickle.load(f)\n",
    "    with open(vocab_path, \"r\") as f:\n",
    "        vocab = json.load(f)\n",
    "    logger.info(f\"Vocabulary loaded: {len(vocab)} tokens\")\n",
    "\n",
    "    # Carica il dataset per il fine-tuning\n",
    "    logger.info(\"Loading new dataset for fine-tuning...\")\n",
    "    with open(config.SMILES_FILE, \"r\") as f:\n",
    "        raw_smiles = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    # Preprocessa il dataset\n",
    "    processed, new_vocab, max_len = process_dataset(raw_smiles)\n",
    "    config.MAX_LENGTH = max_len  # Aggiorna la lunghezza massima se necessario\n",
    "    logger.info(\"Dataset processed for fine-tuning.\")\n",
    "\n",
    "    # Suddividi il dataset in training set e validation set\n",
    "    stratify_labels = [min(len(t), 20) for t in processed]  # Etichette semplificate per stratificazione\n",
    "    try:\n",
    "        counts = np.bincount(stratify_labels)\n",
    "        if np.min(counts[np.nonzero(counts)]) < 2:\n",
    "            logger.warning(\"Layering disabled: Some classes have less than 2 examples.\")\n",
    "            stratify_param = None\n",
    "        else:\n",
    "            stratify_param = stratify_labels\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Errore nella stratificazione: {e}. Disabilito stratificazione.\")\n",
    "        stratify_param = None\n",
    "\n",
    "    train_data, val_data = train_test_split(\n",
    "        processed,\n",
    "        test_size=config.VALID_RATIO,\n",
    "        stratify=stratify_param,\n",
    "        random_state=42\n",
    "    )\n",
    "    logger.info(f\"Training set: {len(train_data)} SMILES, Validation set: {len(val_data)} SMILES\")\n",
    "\n",
    "    # Crea il generatore per il training (curriculum learning)\n",
    "    curriculum_generator = CurriculumSmilesGenerator(tokenized_smiles=train_data, vocab=vocab)\n",
    "    train_dataset = curriculum_generator.get_dataset()\n",
    "\n",
    "    # Crea il generatore per il validation (generazione semplice, senza curriculum)\n",
    "    val_dataset = get_simple_dataset(val_data, vocab)\n",
    "\n",
    "    # Costruisci il modello e carica i pesi pre-addestrati per il fine-tuning\n",
    "    logger.info(\"Model construction and loading of pre-trained weights...\")\n",
    "    model = build_improved_model(len(vocab))\n",
    "    try:\n",
    "        model.load_weights(model_path)\n",
    "        logger.info(\"Pre-trained weights loaded correctly.\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Unable to load pre-trained weights: {e}. Proceeding with random weights.\")\n",
    "\n",
    "    # Definisci i callback per il training\n",
    "    from tensorflow.keras.callbacks import ModelCheckpoint  # Import aggiuntivo per checkpointing\n",
    "    tensorboard_cb = CustomTensorBoard(log_dir=\"logs_finetune\")\n",
    "    monitor_cb = EnhancedTrainingMonitor()\n",
    "    checkpoint_cb = ModelCheckpoint(\n",
    "        filepath=\"best_model.h5\",  # Salva il modello con loss di validazione minimo\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Avvio del fine-tuning (validation viene valutato ad ogni epoca)\n",
    "    logger.info(\"Starting fine-tuning...\")\n",
    "    steps_per_epoch = max(1, len(train_data) // config.BATCH_SIZE)\n",
    "    val_steps = max(1, len(val_data) // config.BATCH_SIZE)\n",
    "\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        epochs=config.EPOCHS,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=val_dataset,\n",
    "        validation_steps=val_steps,\n",
    "        callbacks=[tensorboard_cb, monitor_cb, checkpoint_cb]\n",
    "    )\n",
    "\n",
    "    # (Opzionale) Salva il modello finale dopo l'ultima epoca\n",
    "    final_model_path = model_out_path\n",
    "    model.save(final_model_path)\n",
    "    logger.info(f\"Fine-tuned model saved in {final_model_path}\")\n",
    "    logger.info(\"Best model based on validation loss saved in best_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2189c5b-7e02-441f-bd61-ad278ca44046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a434cdd0-a9a0-4712-82a7-a24e2851d25f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7090efc5-361c-432a-9ef2-63c8d7738a14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126fb9f7-e85a-47e1-a7a4-dd84f67822f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56a15a0-324b-4c6f-aaa5-b2e764051cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900474d1-2164-48af-ad44-1fd26621249d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
