{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b10dd371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 09:40:32.265587: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-11 09:40:32.280845: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749627632.297694 3709216 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749627632.302877 3709216 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749627632.316726 3709216 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749627632.316748 3709216 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749627632.316749 3709216 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749627632.316750 3709216 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-11 09:40:32.322280: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19.0\n",
      "Num GPUs Available:  1\n",
      "Num CPUs Available:  1\n",
      "tf.Tensor([5. 7. 9.], shape=(3,), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1749627634.885485 3709216 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1426 MB memory:  -> device: 0, name: NVIDIA RTX A2000 12GB, pci bus id: 0000:65:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))  # Should print 0\n",
    "print(\"Num CPUs Available: \", len(tf.config.list_physical_devices('CPU')))  # Should print > 0\n",
    "\n",
    "with tf.device('/cpu:0'):  # Explicitly use CPU (optiona¬ß¬ßl but good practice)\n",
    "    a = tf.constant([1.0, 2.0, 3.0])\n",
    "    b = tf.constant([4.0, 5.0, 6.0])\n",
    "    c = a + b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcc15c93-f205-4e93-88e6-a34e3b854fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configurazione Avanzata ---\n",
    "class AdvancedConfig:\n",
    "    SMILES_FILE = '/home/grad/Desktop/pietro/denovo/new/CHEMBL25.tar.xz'  # file con circa 26k SMILES\n",
    "    BATCH_SIZE = 64\n",
    "    EPOCHS = 200\n",
    "    EMBED_DIM = 94\n",
    "    TRANSFORMER_HEADS = 4\n",
    "    TRANSFORMER_LAYERS = 4\n",
    "    FF_DIM = 300\n",
    "    VALID_RATIO = 0.1\n",
    "    TEMPERATURE = 1.0\n",
    "    TEMPERATURE_DECAY = 0.97\n",
    "    GEN_NUM = 1\n",
    "    WARMUP_EPOCHS = 35  # Per il curriculum learning iniziale\n",
    "    MAX_RANDOMIZATIONS = 3\n",
    "    MAX_LENGTH = 100\n",
    "    PRINT_EVERY = 100\n",
    "    DROPOUT_RATE = 0.15\n",
    "    GRADIENT_CLIP = 1.0\n",
    "    L2_REG = 1e-5\n",
    "    LOSS_STABILITY_THRESHOLD = 0.01  # variazione relativa sotto il 1%\n",
    "    CURRICULUM_START_COMPLEXITY = 0   # complessit√† iniziale (0 = molecole molto semplici)\n",
    "    CURRICULUM_COMPLEXITY_STEP = 1    # incremento della soglia di complessit√†\n",
    "    AUGMENT_PROB = 0.1\n",
    "\n",
    "config = AdvancedConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bce96861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "from rdkit.Chem import MolFromSmiles, MolToSmiles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, GRU, Dense, Bidirectional, \n",
    "    LayerNormalization, Dropout, Attention, Concatenate,\n",
    "    Layer, Multiply, Masking, RepeatVector,  GlobalAveragePooling1D\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ModelCheckpoint, \n",
    "    LearningRateScheduler, Callback\n",
    ")\n",
    "import re\n",
    "import logging\n",
    "from typing import List, Optional, Tuple\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MolFromSmiles, MolToSmiles, SanitizeMol, SanitizeFlags\n",
    "from rdkit.Chem.rdmolops import AssignStereochemistry\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Embedding, Input, LayerNormalization, MultiHeadAttention, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from threading import Lock\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Per grafici 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "755fe76d-6c19-4a1e-90fa-608500847321",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 09:40:35,217 [INFO] üöÄ Avvio Training Transformer per SMILES con tokenizzazione BPE, data augmentation e curriculum learning basato sulla complessit√†\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xfd in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 366\u001b[0m\n\u001b[1;32m    364\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müöÄ Avvio Training Transformer per SMILES con tokenizzazione BPE, data augmentation e curriculum learning basato sulla complessit√†\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(config\u001b[38;5;241m.\u001b[39mSMILES_FILE) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 366\u001b[0m     raw_smiles \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[1;32m    367\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müîç Validazione SMILES...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    368\u001b[0m valid_smiles \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[4], line 366\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    364\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müöÄ Avvio Training Transformer per SMILES con tokenizzazione BPE, data augmentation e curriculum learning basato sulla complessit√†\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(config\u001b[38;5;241m.\u001b[39mSMILES_FILE) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 366\u001b[0m     raw_smiles \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[1;32m    367\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müîç Validazione SMILES...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    368\u001b[0m valid_smiles \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/smiles-transformer/lib/python3.10/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m data[consumed:]\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xfd in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Configurazione del logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Funzione per calcolare la complessit√† di una SMILES ---\n",
    "def compute_complexity_from_tokens(tokens: List[str]) -> int:\n",
    "    \"\"\"\n",
    "    Calcola la complessit√† di una SMILES come la somma del numero di anelli (ottenuti con GetSSSR)\n",
    "    e del numero di ramificazioni (conteggio delle parentesi aperte).\n",
    "    \"\"\"\n",
    "    smiles = ''.join(tokens)\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if not mol:\n",
    "            return float('inf')\n",
    "        num_rings = Chem.GetSSSR(mol)\n",
    "        num_branches = smiles.count('(')\n",
    "        return num_rings + num_branches\n",
    "    except Exception:\n",
    "        return float('inf')\n",
    "\n",
    "# --- Funzioni di Preprocessing ---\n",
    "def randomize_smiles(smiles: str, num_versions: int = 3) -> List[str]:\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if not mol:\n",
    "        return []\n",
    "    randomized = []\n",
    "    for _ in range(num_versions):\n",
    "        try:\n",
    "            new_smiles = Chem.MolToSmiles(mol, doRandom=True, canonical=False)\n",
    "            if new_smiles:\n",
    "                randomized.append(new_smiles)\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Randomization error: {e}\")\n",
    "            continue\n",
    "    return randomized\n",
    "\n",
    "def validate_and_fix_smiles(smiles: str) -> str:\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        try:\n",
    "            Chem.Kekulize(mol, clearAromaticFlags=True)\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Kekulization error in SMILES {smiles}: {e}\")\n",
    "            return None\n",
    "        return Chem.MolToSmiles(mol, canonical=True, isomericSmiles=False)\n",
    "    except Exception as e:\n",
    "        logger.debug(f\"Parsing/sanitization error in SMILES {smiles}: {e}\")\n",
    "        return None\n",
    "\n",
    "def robust_tokenize(smiles: str) -> list:\n",
    "    pattern = (\n",
    "        r\"(\\[[^\\[\\]]{1,6}\\]|\"                 # atomi in parentesi quadre\n",
    "        r\"Br|Cl|Si|Na|Mg|Mn|Ca|Fe|Zn|Se|Li|K|Al|B|\"  # elementi multi-char\n",
    "        r\"R[0-9]|r[0-9]|a[0-9]|\"             # ring labels\n",
    "        r\"[A-Za-z0-9@+\\-\\\\\\/\\(\\)=#\\$\\.\\%,])\"  # singoli caratteri, incluso '%'\n",
    "    )\n",
    "    tokens = re.findall(pattern, smiles)\n",
    "    tokens = re.findall(pattern, smiles)\n",
    "    stack = []\n",
    "    for t in tokens:\n",
    "        if t.startswith('['):\n",
    "            stack.append(t)\n",
    "        if t.endswith(']') and not stack:\n",
    "            return []\n",
    "        if t.endswith(']'):\n",
    "            stack.pop()\n",
    "    try:\n",
    "        if not stack and Chem.MolFromSmiles(''.join(tokens)):\n",
    "            return tokens\n",
    "    except Exception as e:\n",
    "        logger.debug(f\"Tokenization error: {e}\")\n",
    "        return []\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def process_dataset(data: List[str]) -> Tuple[List[List[str]], List[str], int]:\n",
    "    processed = []\n",
    "    all_tokens = set()\n",
    "    for s in data:\n",
    "        fixed = validate_and_fix_smiles(s)\n",
    "        if not fixed:\n",
    "            continue\n",
    "        tokens = robust_tokenize(fixed)\n",
    "        if tokens and 3 <= len(tokens) <= config.MAX_LENGTH - 2:\n",
    "            processed.append(tokens)\n",
    "            all_tokens.update(tokens)\n",
    "    vocab = ['<PAD>', '<START>', '<END>'] + sorted(all_tokens)\n",
    "    lengths = [len(t) for t in processed]\n",
    "    max_len = min(int(np.percentile(lengths, 99)) + 2, config.MAX_LENGTH) if processed else config.MAX_LENGTH\n",
    "    logger.info(f\"Processed SMILES: {len(processed)}/{len(data)}\")\n",
    "    logger.info(f\"Unique tokens: {len(all_tokens)}\")\n",
    "    logger.info(f\"Max length: {max_len}\")\n",
    "    return processed, vocab, max_len\n",
    "\n",
    "# --- Componenti del Modello ---\n",
    "class DynamicPositionalEncoding(Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "    def build(self, input_shape):\n",
    "        max_seq_len = input_shape[1]\n",
    "        pos = np.arange(max_seq_len)[:, np.newaxis]\n",
    "        i = np.arange(self.embed_dim)[np.newaxis, :]\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(self.embed_dim))\n",
    "        angle_rads = pos * angle_rates\n",
    "        angle_rads[:, 0::2] = tf.math.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = tf.math.cos(angle_rads[:, 1::2])\n",
    "        self.pos_encoding = tf.cast(angle_rads[np.newaxis, ...], dtype=tf.float32)\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        base_config.update({\"embed_dim\": self.embed_dim})\n",
    "        return base_config\n",
    "\n",
    "class ImprovedTransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim, rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.rate = rate\n",
    "        self.mha = MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embed_dim,\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(config.L2_REG),\n",
    "            dropout=rate\n",
    "        )\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ffn_dim, activation=\"gelu\", kernel_regularizer=tf.keras.regularizers.l2(config.L2_REG)),\n",
    "            Dense(embed_dim, kernel_regularizer=tf.keras.regularizers.l2(config.L2_REG))\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        causal_mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        attn_output = self.mha(inputs, inputs, attention_mask=causal_mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        base_config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"ffn_dim\": self.ffn_dim,\n",
    "            \"rate\": self.rate\n",
    "        })\n",
    "        return base_config\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, embed_dim, warmup_steps=10000):\n",
    "        super().__init__()\n",
    "        self.embed_dim = tf.cast(embed_dim, tf.float32)\n",
    "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32) + 1e-9\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.embed_dim) * tf.math.minimum(arg1, arg2)\n",
    "    def get_config(self):\n",
    "        return {\"embed_dim\": self.embed_dim.numpy(), \"warmup_steps\": self.warmup_steps.numpy()}\n",
    "\n",
    "def build_improved_model(vocab_size: int) -> Model:\n",
    "    inputs = Input(shape=(config.MAX_LENGTH,))\n",
    "    x = Embedding(vocab_size, config.EMBED_DIM, mask_zero=True)(inputs)\n",
    "    x = DynamicPositionalEncoding(config.EMBED_DIM)(x)\n",
    "    x = Dropout(config.DROPOUT_RATE)(x)\n",
    "    for _ in range(config.TRANSFORMER_LAYERS):\n",
    "        x = ImprovedTransformerBlock(config.EMBED_DIM, config.TRANSFORMER_HEADS, config.FF_DIM, rate=config.DROPOUT_RATE)(x)\n",
    "    outputs = Dense(vocab_size)(x)\n",
    "    def smoothed_loss(y_true, y_pred):\n",
    "        # cast a interi perch√© tf.nn.sparse_* vuole etichette int32 o int64\n",
    "        y_true_int = tf.cast(y_true, tf.int32)\n",
    "        mask = tf.cast(tf.math.not_equal(y_true, 0), tf.float32)\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true_int, logits=y_pred)\n",
    "        return tf.reduce_sum(loss * mask) / (tf.reduce_sum(mask) + 1e-9)\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=CustomSchedule(config.EMBED_DIM),\n",
    "        clipnorm=config.GRADIENT_CLIP\n",
    "    )\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=optimizer, loss=smoothed_loss)\n",
    "    return model\n",
    "\n",
    "# --- Generatori Dati e Utilities ---\n",
    "class ThreadSafeIterator:\n",
    "    def __init__(self, iterator):\n",
    "        self.iterator = iterator\n",
    "        self.lock = Lock()\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        with self.lock:\n",
    "            return next(self.iterator)\n",
    "\n",
    "def threadsafe_generator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        return ThreadSafeIterator(func(*args, **kwargs))\n",
    "    return wrapper\n",
    "\n",
    "class CurriculumSmilesGenerator:\n",
    "    \"\"\"\n",
    "    Generatore che implementa il curriculum learning basato sulla complessit√†:\n",
    "      - Durante le prime WARMUP_EPOCHS la soglia di complessit√† aumenta.\n",
    "      - Se la loss si stabilizza (variazione < LOSS_STABILITY_THRESHOLD), la soglia viene incrementata.\n",
    "      - Applica data augmentation con probabilit√† AUGMENT_PROB.\n",
    "      - Le molecole non valide vengono scartate.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenized_smiles: List[List[str]], vocab: List[str]):\n",
    "        self.char2idx = {c: i for i, c in enumerate(vocab)}\n",
    "        self.idx2char = {i: c for c, i in self.char2idx.items()}\n",
    "        self.original_data = []\n",
    "        for tokens in tokenized_smiles:\n",
    "            comp = compute_complexity_from_tokens(tokens)\n",
    "            fixed = validate_and_fix_smiles(''.join(tokens))\n",
    "            if fixed is None:\n",
    "                continue\n",
    "            self.original_data.append((tokens, comp))\n",
    "        if self.original_data:\n",
    "            valid_comps = [comp for _, comp in self.original_data if comp != float('inf')]\n",
    "            self.max_complexity = max(valid_comps) if valid_comps else 0\n",
    "        else:\n",
    "            self.max_complexity = 0\n",
    "        self.current_complexity = config.CURRICULUM_START_COMPLEXITY\n",
    "        self.available_data = self._filter_data()\n",
    "        self.train_smiles = {''.join(tokens) for tokens, _ in self.original_data}\n",
    "        self.lock = Lock()\n",
    "    \n",
    "    def _filter_data(self):\n",
    "        filtered = [tokens for tokens, comp in self.original_data if comp <= self.current_complexity]\n",
    "        return filtered or [tokens for tokens, comp in self.original_data]\n",
    "    \n",
    "    def update_complexity(self, epoch: int, loss_diff: float = None):\n",
    "        with self.lock:\n",
    "            if loss_diff is not None and loss_diff < config.LOSS_STABILITY_THRESHOLD:\n",
    "                self.current_complexity = min(self.current_complexity + config.CURRICULUM_COMPLEXITY_STEP, self.max_complexity)\n",
    "            else:\n",
    "                if epoch <= config.WARMUP_EPOCHS:\n",
    "                    increment = int((self.max_complexity - config.CURRICULUM_START_COMPLEXITY) * (epoch / config.WARMUP_EPOCHS))\n",
    "                    self.current_complexity = config.CURRICULUM_START_COMPLEXITY + increment\n",
    "                else:\n",
    "                    self.current_complexity = self.max_complexity\n",
    "            self.available_data = self._filter_data()\n",
    "            if not self.available_data:\n",
    "                self.available_data = [tokens for tokens, comp in self.original_data]\n",
    "                logger.warning(\"Reset available_data to original\")\n",
    "    \n",
    "    @threadsafe_generator\n",
    "    def __call__(self):\n",
    "        while True:\n",
    "            inputs = np.full((config.BATCH_SIZE, config.MAX_LENGTH), self.char2idx['<PAD>'], dtype=np.int32)\n",
    "            targets = np.full_like(inputs, self.char2idx['<PAD>'])\n",
    "            for i in range(config.BATCH_SIZE):\n",
    "                with self.lock:\n",
    "                    try:\n",
    "                        tokens = random.choice(self.available_data)\n",
    "                    except IndexError:\n",
    "                        self.available_data = [tokens for tokens, comp in self.original_data]\n",
    "                        tokens = random.choice(self.available_data)\n",
    "                if random.random() < config.AUGMENT_PROB:\n",
    "                    try:\n",
    "                        augmented = randomize_smiles(''.join(tokens))\n",
    "                        if augmented:\n",
    "                            new_tokens = robust_tokenize(random.choice(augmented)) or tokens\n",
    "                            tokens = new_tokens\n",
    "                    except Exception as e:\n",
    "                        logger.debug(f\"Augmentation error: {e}\")\n",
    "                seq = ['<START>'] + tokens + ['<END>']\n",
    "                padded = (seq + ['<PAD>'] * config.MAX_LENGTH)[:config.MAX_LENGTH]\n",
    "                inputs[i] = [self.char2idx.get(t, self.char2idx['<PAD>']) for t in padded]\n",
    "                targets[i, :-1] = inputs[i][1:]\n",
    "                targets[i, -1] = self.char2idx['<PAD>']\n",
    "            yield inputs, targets\n",
    "\n",
    "    def get_dataset(self):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            self.__call__,\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=(config.BATCH_SIZE, config.MAX_LENGTH), dtype=tf.int32),\n",
    "                tf.TensorSpec(shape=(config.BATCH_SIZE, config.MAX_LENGTH), dtype=tf.int32)\n",
    "            )\n",
    "        ).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# --- Callback per il monitoraggio ---\n",
    "class CustomTensorBoard(tf.keras.callbacks.TensorBoard):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        lr = self.model.optimizer.learning_rate\n",
    "        if isinstance(lr, tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "            logs['lr'] = lr(epoch).numpy()\n",
    "        else:\n",
    "            logs['lr'] = lr.numpy()\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "\n",
    "class EnhancedTrainingMonitor(Callback):\n",
    "    def __init__(self, val_gen: CurriculumSmilesGenerator):\n",
    "        super().__init__()\n",
    "        self.val_gen = val_gen\n",
    "        self.best_val_loss = np.inf\n",
    "        self.prev_val_loss = None\n",
    "    def generate_smiles(self, num: int) -> Tuple[List[str], List[str]]:\n",
    "        generated, valid = [], []\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Il modello non √® stato assegnato al callback!\")\n",
    "        input_seq = np.full((1, config.MAX_LENGTH), self.val_gen.char2idx['<PAD>'], dtype=np.int32)\n",
    "        input_seq[0, 0] = self.val_gen.char2idx['<START>']\n",
    "        for _ in range(num):\n",
    "            for t in range(1, config.MAX_LENGTH):\n",
    "                logits = self.model(input_seq, training=False)[0, t-1]\n",
    "                probs = tf.nn.softmax(logits / config.TEMPERATURE).numpy()\n",
    "                valid_indices = [i for i, tok in self.val_gen.idx2char.items() \n",
    "                                 if re.match(r'^([A-Za-z0-9@#\\[\\]()+\\-\\\\/%=:.,]|<\\w+>)$', tok)]\n",
    "                probs[[i for i in range(len(probs)) if i not in valid_indices]] = 0\n",
    "                if np.sum(probs) == 0:\n",
    "                    break\n",
    "                probs /= np.sum(probs)\n",
    "                sampled = np.random.choice(len(probs), p=probs)\n",
    "                input_seq[0, t] = sampled\n",
    "                if sampled == self.val_gen.char2idx['<END>']:\n",
    "                    break\n",
    "            raw = ''.join([self.val_gen.idx2char[i] for i in input_seq[0]\n",
    "                           if i not in {self.val_gen.char2idx.get('<PAD>'), self.val_gen.char2idx['<END>']}][1:])\n",
    "            final = validate_and_fix_smiles(raw) or raw\n",
    "            generated.append(final)\n",
    "            if Chem.MolFromSmiles(final):\n",
    "                valid.append(final)\n",
    "            input_seq = np.full((1, config.MAX_LENGTH), self.val_gen.char2idx['<PAD>'], dtype=np.int32)\n",
    "            input_seq[0, 0] = self.val_gen.char2idx['<START>']\n",
    "        return generated, valid\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % config.PRINT_EVERY == 0:\n",
    "            generated, valid = self.generate_smiles(config.GEN_NUM)\n",
    "            validity = len(valid) / config.GEN_NUM\n",
    "            unique = len(set(valid))\n",
    "            novel = len([s for s in valid if s not in self.val_gen.train_smiles])\n",
    "            logger.info(f\"\\nüß™ Epoca {epoch+1}:\")\n",
    "            logger.info(f\"Loss Training: {logs.get('loss', 'N/A'):.4f} - Loss Validation: {logs.get('val_loss', 'N/A'):.4f}\")\n",
    "            logger.info(f\"Validit√†: {validity:.1%}\")\n",
    "            logger.info(f\"Unicit√†: {unique}/{len(valid)}\")\n",
    "            logger.info(f\"Novit√†: {novel}/{len(valid)}\")\n",
    "            if valid:\n",
    "                logger.info(\"Esempi:\")\n",
    "                for s in valid[:3]:\n",
    "                    logger.info(f\"- {s}\")\n",
    "            with open(\"valid_generated_smiles.txt\", \"w\") as f:\n",
    "                for s in valid:\n",
    "                    f.write(s + \"\\n\")\n",
    "\n",
    "# --- Main ---\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"üöÄ Avvio Training Transformer per SMILES con tokenizzazione BPE, data augmentation e curriculum learning basato sulla complessit√†\")\n",
    "    with open(config.SMILES_FILE) as f:\n",
    "        raw_smiles = [line.strip() for line in f if line.strip()]\n",
    "    logger.info(\"üîç Validazione SMILES...\")\n",
    "    valid_smiles = []\n",
    "    for idx, s in enumerate(raw_smiles):\n",
    "        if idx % 5000 == 0:\n",
    "            logger.info(f\"Processati {idx}/{len(raw_smiles)}\")\n",
    "        fixed = validate_and_fix_smiles(s)\n",
    "        if fixed and 3 <= len(fixed) <= config.MAX_LENGTH:\n",
    "            valid_smiles.append(fixed)\n",
    "    processed, vocab, max_len = process_dataset(valid_smiles)\n",
    "    config.MAX_LENGTH = max_len\n",
    "    logger.info(\"Vocabolario creato:\")\n",
    "    logger.info(vocab)\n",
    "    \n",
    "    import pickle\n",
    "    with open('char2idx.pkl', 'wb') as f:\n",
    "       pickle.dump({char: idx for idx, char in enumerate(vocab)}, f)\n",
    "    with open('idx2char.pkl', 'wb') as f:\n",
    "       pickle.dump({idx: char for idx, char in enumerate(vocab)}, f)\n",
    "    import json\n",
    "    with open(\"vocab.json\", \"w\") as f:\n",
    "        json.dump(vocab, f)\n",
    "    logger.info(\"Mappature e vocabolario salvati su disco.\")\n",
    "    \n",
    "    stratify_labels = [min(len(t), 20) for t in processed]\n",
    "    try:\n",
    "        counts = np.bincount(stratify_labels)\n",
    "        if np.min(counts[np.nonzero(counts)]) < 2:\n",
    "            logger.warning(\"Stratificazione disabilitata: alcune classi hanno meno di 2 esempi.\")\n",
    "            stratify_param = None\n",
    "        else:\n",
    "            stratify_param = stratify_labels\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Errore nella stratificazione: {e}. Disabilito stratificazione.\")\n",
    "        stratify_param = None\n",
    "\n",
    "    train_data, val_data = train_test_split(\n",
    "        processed,\n",
    "        test_size=config.VALID_RATIO,\n",
    "        stratify=stratify_param,\n",
    "        random_state=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fdfed7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    model = build_improved_model(len(vocab))\n",
    "    train_gen = CurriculumSmilesGenerator(train_data, vocab)\n",
    "    val_gen = CurriculumSmilesGenerator(val_data, vocab)\n",
    "    \n",
    "    train_gen.update_complexity(0)\n",
    "    val_gen.update_complexity(0)\n",
    "    \n",
    "    monitor_callback = EnhancedTrainingMonitor(val_gen)\n",
    "    \n",
    "    callbacks = [\n",
    "        monitor_callback,\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6),\n",
    "        CustomTensorBoard(log_dir='./logs'),\n",
    "        tf.keras.callbacks.ModelCheckpoint('diff_scaffold_2.h5', save_best_only=True, monitor='val_loss')\n",
    "    ]\n",
    "    \n",
    "    steps_per_epoch = len(train_data) // config.BATCH_SIZE\n",
    "    val_steps = len(val_data) // config.BATCH_SIZE\n",
    "    total_epochs = config.EPOCHS\n",
    "    start_training = time.time()\n",
    "    \n",
    "    for epoch in range(1, total_epochs + 1):\n",
    "        epoch_start = time.time()\n",
    "        current_history = model.fit(\n",
    "            train_gen.get_dataset(),\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            callbacks=callbacks,\n",
    "            epochs=1,\n",
    "            validation_data=val_gen.get_dataset(),\n",
    "            validation_steps=val_steps,\n",
    "            verbose=2,\n",
    "        )\n",
    "        epoch_duration = time.time() - epoch_start\n",
    "        current_val_loss = current_history.history['val_loss'][0]\n",
    "        if epoch > 1:\n",
    "            loss_diff = abs(current_val_loss - prev_val_loss) / prev_val_loss\n",
    "        else:\n",
    "            loss_diff = None\n",
    "        prev_val_loss = current_val_loss\n",
    "\n",
    "        train_gen.update_complexity(epoch, loss_diff=loss_diff)\n",
    "        val_gen.update_complexity(epoch, loss_diff=loss_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe53fbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dopo il training, per generare SMILES e calcolare le metriche:\n",
    "generated, valid = monitor_callback.generate_smiles(10000)  # genera 1000 SMILES\n",
    "\n",
    "# Calcola validit√†\n",
    "validity = len(valid) / 10000\n",
    "\n",
    "# Calcola unicit√† (solo tra quelle valide)\n",
    "unique_valid = len(set(valid))\n",
    "uniqueness = unique_valid / len(valid) if valid else 0\n",
    "\n",
    "print(f\"Validit√†: {validity*100:.2f}%\")\n",
    "print(f\"Unicit√†: {uniqueness*100:.2f}%\")\n",
    "\n",
    "# Per la diversit√†, puoi per esempio calcolare le fingerprint e misurare la similarit√† media.\n",
    "# Questo √® un esempio con RDKit (assicurati di avere rdkit installato):\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, DataStructs\n",
    "\n",
    "fps = []\n",
    "for s in valid:\n",
    "    mol = Chem.MolFromSmiles(s)\n",
    "    if mol:\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2)\n",
    "        fps.append(fp)\n",
    "\n",
    "# Calcola la similarit√† media tra tutte le coppie\n",
    "similarities = []\n",
    "for i in range(len(fps)):\n",
    "    for j in range(i+1, len(fps)):\n",
    "        sim = DataStructs.TanimotoSimilarity(fps[i], fps[j])\n",
    "        similarities.append(sim)\n",
    "        \n",
    "avg_similarity = np.mean(similarities) if similarities else 0\n",
    "diversity = 1 - avg_similarity\n",
    "\n",
    "print(f\"Diversit√† (1 - similarit√† media): {diversity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f118703e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c81ba74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
