{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "799d102f-5975-4d24-aadb-912545b24343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/grad/Desktop/pietro/denovo/ab/ab2\n"
     ]
    }
   ],
   "source": [
    "cd /home/grad/Desktop/pietro/denovo/ab/ab2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b10dd371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 10:01:20.930370: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-24 10:01:20.944915: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750752080.960858 3063298 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750752080.965642 3063298 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750752080.977701 3063298 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750752080.977717 3063298 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750752080.977718 3063298 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750752080.977719 3063298 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-24 10:01:20.981488: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19.0\n",
      "Num GPUs Available:  1\n",
      "Num CPUs Available:  1\n",
      "tf.Tensor([5. 7. 9.], shape=(3,), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750752083.425083 3063298 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1584 MB memory:  -> device: 0, name: NVIDIA RTX A2000 12GB, pci bus id: 0000:65:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))  # Should print 0\n",
    "print(\"Num CPUs Available: \", len(tf.config.list_physical_devices('CPU')))  # Should print > 0\n",
    "\n",
    "with tf.device('/cpu:0'):  # Explicitly use CPU (optiona¬ß¬ßl but good practice)\n",
    "    a = tf.constant([1.0, 2.0, 3.0])\n",
    "    b = tf.constant([4.0, 5.0, 6.0])\n",
    "    c = a + b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcc15c93-f205-4e93-88e6-a34e3b854fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configurazione Avanzata ---\n",
    "class AdvancedConfig:\n",
    "    SMILES_FILE = '/home/grad/Desktop/pietro/denovo/new/CHEMBL25.smi'  # file con circa 26k SMILES\n",
    "    BATCH_SIZE = 64\n",
    "    EPOCHS = 20\n",
    "    EMBED_DIM = 94\n",
    "    TRANSFORMER_HEADS = 2\n",
    "    TRANSFORMER_LAYERS = 2\n",
    "    FF_DIM = 200\n",
    "    VALID_RATIO = 0.1\n",
    "    TEMPERATURE = 1.0\n",
    "    TEMPERATURE_DECAY = 0.97\n",
    "    GEN_NUM = 1\n",
    "    WARMUP_EPOCHS = 10  # Per il curriculum learning iniziale\n",
    "    MAX_RANDOMIZATIONS = 3\n",
    "    MAX_LENGTH = 100\n",
    "    PRINT_EVERY = 100\n",
    "    DROPOUT_RATE = 0.15\n",
    "    GRADIENT_CLIP = 1.0\n",
    "    L2_REG = 1e-5\n",
    "    LOSS_STABILITY_THRESHOLD = 0.01  # variazione relativa sotto il 1%\n",
    "    CURRICULUM_START_COMPLEXITY = 5   # complessit√† iniziale (0 = molecole molto semplici)\n",
    "    CURRICULUM_COMPLEXITY_STEP = 1    # incremento della soglia di complessit√†\n",
    "    AUGMENT_PROB = 0.3\n",
    "\n",
    "config = AdvancedConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bce96861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "from rdkit.Chem import MolFromSmiles, MolToSmiles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, GRU, Dense, Bidirectional, \n",
    "    LayerNormalization, Dropout, Attention, Concatenate,\n",
    "    Layer, Multiply, Masking, RepeatVector,  GlobalAveragePooling1D\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ModelCheckpoint, \n",
    "    LearningRateScheduler, Callback\n",
    ")\n",
    "import re\n",
    "import logging\n",
    "from typing import List, Optional, Tuple\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MolFromSmiles, MolToSmiles, SanitizeMol, SanitizeFlags\n",
    "from rdkit.Chem.rdmolops import AssignStereochemistry\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Embedding, Input, LayerNormalization, MultiHeadAttention, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from threading import Lock\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Per grafici 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "755fe76d-6c19-4a1e-90fa-608500847321",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 10:01:26,094 [INFO] üöÄ Avvio Training Transformer per SMILES con tokenizzazione BPE, data augmentation e curriculum learning basato sulla complessit√†\n",
      "2025-06-24 10:01:26,367 [INFO] üîç Validazione SMILES...\n",
      "2025-06-24 10:01:26,367 [INFO] Processati 0/1213651\n",
      "2025-06-24 10:01:27,706 [INFO] Processati 5000/1213651\n",
      "2025-06-24 10:01:29,524 [INFO] Processati 10000/1213651\n",
      "2025-06-24 10:01:31,066 [INFO] Processati 15000/1213651\n",
      "2025-06-24 10:01:32,510 [INFO] Processati 20000/1213651\n",
      "2025-06-24 10:01:33,842 [INFO] Processati 25000/1213651\n",
      "2025-06-24 10:01:35,147 [INFO] Processati 30000/1213651\n",
      "2025-06-24 10:01:36,516 [INFO] Processati 35000/1213651\n",
      "2025-06-24 10:01:37,975 [INFO] Processati 40000/1213651\n",
      "2025-06-24 10:01:39,260 [INFO] Processati 45000/1213651\n",
      "2025-06-24 10:01:40,628 [INFO] Processati 50000/1213651\n",
      "2025-06-24 10:01:42,031 [INFO] Processati 55000/1213651\n",
      "2025-06-24 10:01:43,455 [INFO] Processati 60000/1213651\n",
      "2025-06-24 10:01:44,873 [INFO] Processati 65000/1213651\n",
      "2025-06-24 10:01:46,271 [INFO] Processati 70000/1213651\n",
      "2025-06-24 10:01:47,591 [INFO] Processati 75000/1213651\n",
      "2025-06-24 10:01:48,845 [INFO] Processati 80000/1213651\n",
      "2025-06-24 10:01:50,144 [INFO] Processati 85000/1213651\n",
      "2025-06-24 10:01:51,381 [INFO] Processati 90000/1213651\n",
      "2025-06-24 10:01:52,640 [INFO] Processati 95000/1213651\n",
      "2025-06-24 10:01:53,922 [INFO] Processati 100000/1213651\n",
      "2025-06-24 10:01:55,144 [INFO] Processati 105000/1213651\n",
      "2025-06-24 10:01:56,478 [INFO] Processati 110000/1213651\n",
      "2025-06-24 10:01:57,740 [INFO] Processati 115000/1213651\n",
      "2025-06-24 10:01:59,025 [INFO] Processati 120000/1213651\n",
      "2025-06-24 10:02:00,292 [INFO] Processati 125000/1213651\n",
      "2025-06-24 10:02:01,567 [INFO] Processati 130000/1213651\n",
      "2025-06-24 10:02:02,796 [INFO] Processati 135000/1213651\n",
      "2025-06-24 10:02:04,022 [INFO] Processati 140000/1213651\n",
      "2025-06-24 10:02:05,245 [INFO] Processati 145000/1213651\n",
      "2025-06-24 10:02:06,493 [INFO] Processati 150000/1213651\n",
      "2025-06-24 10:02:07,836 [INFO] Processati 155000/1213651\n",
      "2025-06-24 10:02:09,066 [INFO] Processati 160000/1213651\n",
      "2025-06-24 10:02:10,283 [INFO] Processati 165000/1213651\n",
      "2025-06-24 10:02:11,561 [INFO] Processati 170000/1213651\n",
      "2025-06-24 10:02:12,951 [INFO] Processati 175000/1213651\n",
      "2025-06-24 10:02:14,214 [INFO] Processati 180000/1213651\n",
      "2025-06-24 10:02:15,490 [INFO] Processati 185000/1213651\n",
      "2025-06-24 10:02:16,748 [INFO] Processati 190000/1213651\n",
      "2025-06-24 10:02:18,057 [INFO] Processati 195000/1213651\n",
      "2025-06-24 10:02:19,343 [INFO] Processati 200000/1213651\n",
      "2025-06-24 10:02:20,609 [INFO] Processati 205000/1213651\n",
      "2025-06-24 10:02:21,914 [INFO] Processati 210000/1213651\n",
      "2025-06-24 10:02:23,178 [INFO] Processati 215000/1213651\n",
      "2025-06-24 10:02:24,492 [INFO] Processati 220000/1213651\n",
      "2025-06-24 10:02:25,741 [INFO] Processati 225000/1213651\n",
      "2025-06-24 10:02:27,025 [INFO] Processati 230000/1213651\n",
      "2025-06-24 10:02:28,290 [INFO] Processati 235000/1213651\n",
      "2025-06-24 10:02:29,597 [INFO] Processati 240000/1213651\n",
      "2025-06-24 10:02:30,845 [INFO] Processati 245000/1213651\n",
      "2025-06-24 10:02:32,079 [INFO] Processati 250000/1213651\n",
      "2025-06-24 10:02:33,300 [INFO] Processati 255000/1213651\n",
      "2025-06-24 10:02:34,706 [INFO] Processati 260000/1213651\n",
      "2025-06-24 10:02:36,099 [INFO] Processati 265000/1213651\n",
      "2025-06-24 10:02:37,464 [INFO] Processati 270000/1213651\n",
      "2025-06-24 10:02:38,882 [INFO] Processati 275000/1213651\n",
      "2025-06-24 10:02:40,404 [INFO] Processati 280000/1213651\n",
      "2025-06-24 10:02:41,739 [INFO] Processati 285000/1213651\n",
      "2025-06-24 10:02:43,017 [INFO] Processati 290000/1213651\n",
      "2025-06-24 10:02:44,323 [INFO] Processati 295000/1213651\n",
      "2025-06-24 10:02:45,583 [INFO] Processati 300000/1213651\n",
      "2025-06-24 10:02:46,968 [INFO] Processati 305000/1213651\n",
      "2025-06-24 10:02:48,354 [INFO] Processati 310000/1213651\n",
      "2025-06-24 10:02:49,764 [INFO] Processati 315000/1213651\n",
      "2025-06-24 10:02:51,185 [INFO] Processati 320000/1213651\n",
      "2025-06-24 10:02:52,589 [INFO] Processati 325000/1213651\n",
      "2025-06-24 10:02:53,955 [INFO] Processati 330000/1213651\n",
      "2025-06-24 10:02:55,255 [INFO] Processati 335000/1213651\n",
      "2025-06-24 10:02:56,467 [INFO] Processati 340000/1213651\n",
      "2025-06-24 10:02:57,751 [INFO] Processati 345000/1213651\n",
      "2025-06-24 10:02:59,082 [INFO] Processati 350000/1213651\n",
      "2025-06-24 10:03:00,367 [INFO] Processati 355000/1213651\n",
      "2025-06-24 10:03:01,681 [INFO] Processati 360000/1213651\n",
      "2025-06-24 10:03:03,008 [INFO] Processati 365000/1213651\n",
      "2025-06-24 10:03:04,392 [INFO] Processati 370000/1213651\n",
      "2025-06-24 10:03:05,819 [INFO] Processati 375000/1213651\n",
      "2025-06-24 10:03:07,331 [INFO] Processati 380000/1213651\n",
      "2025-06-24 10:03:08,786 [INFO] Processati 385000/1213651\n",
      "2025-06-24 10:03:10,106 [INFO] Processati 390000/1213651\n",
      "2025-06-24 10:03:11,441 [INFO] Processati 395000/1213651\n",
      "2025-06-24 10:03:12,741 [INFO] Processati 400000/1213651\n",
      "2025-06-24 10:03:14,027 [INFO] Processati 405000/1213651\n",
      "2025-06-24 10:03:15,351 [INFO] Processati 410000/1213651\n",
      "2025-06-24 10:03:16,767 [INFO] Processati 415000/1213651\n",
      "2025-06-24 10:03:18,134 [INFO] Processati 420000/1213651\n",
      "2025-06-24 10:03:19,476 [INFO] Processati 425000/1213651\n",
      "2025-06-24 10:03:20,866 [INFO] Processati 430000/1213651\n",
      "2025-06-24 10:03:22,209 [INFO] Processati 435000/1213651\n",
      "2025-06-24 10:03:23,450 [INFO] Processati 440000/1213651\n",
      "2025-06-24 10:03:24,816 [INFO] Processati 445000/1213651\n",
      "2025-06-24 10:03:26,172 [INFO] Processati 450000/1213651\n",
      "2025-06-24 10:03:27,532 [INFO] Processati 455000/1213651\n",
      "2025-06-24 10:03:28,898 [INFO] Processati 460000/1213651\n",
      "2025-06-24 10:03:30,273 [INFO] Processati 465000/1213651\n",
      "2025-06-24 10:03:31,646 [INFO] Processati 470000/1213651\n",
      "2025-06-24 10:03:32,951 [INFO] Processati 475000/1213651\n",
      "2025-06-24 10:03:34,256 [INFO] Processati 480000/1213651\n",
      "2025-06-24 10:03:35,574 [INFO] Processati 485000/1213651\n",
      "2025-06-24 10:03:36,752 [INFO] Processati 490000/1213651\n",
      "2025-06-24 10:03:37,982 [INFO] Processati 495000/1213651\n",
      "2025-06-24 10:03:39,389 [INFO] Processati 500000/1213651\n",
      "2025-06-24 10:03:40,775 [INFO] Processati 505000/1213651\n",
      "2025-06-24 10:03:42,181 [INFO] Processati 510000/1213651\n",
      "2025-06-24 10:03:43,555 [INFO] Processati 515000/1213651\n",
      "2025-06-24 10:03:44,927 [INFO] Processati 520000/1213651\n",
      "2025-06-24 10:03:46,317 [INFO] Processati 525000/1213651\n",
      "2025-06-24 10:03:47,730 [INFO] Processati 530000/1213651\n",
      "2025-06-24 10:03:49,131 [INFO] Processati 535000/1213651\n",
      "2025-06-24 10:03:50,513 [INFO] Processati 540000/1213651\n",
      "2025-06-24 10:03:51,866 [INFO] Processati 545000/1213651\n",
      "2025-06-24 10:03:53,289 [INFO] Processati 550000/1213651\n",
      "2025-06-24 10:03:54,690 [INFO] Processati 555000/1213651\n",
      "2025-06-24 10:03:56,069 [INFO] Processati 560000/1213651\n",
      "2025-06-24 10:03:57,442 [INFO] Processati 565000/1213651\n",
      "2025-06-24 10:03:58,840 [INFO] Processati 570000/1213651\n",
      "2025-06-24 10:04:00,166 [INFO] Processati 575000/1213651\n",
      "2025-06-24 10:04:01,507 [INFO] Processati 580000/1213651\n",
      "2025-06-24 10:04:02,833 [INFO] Processati 585000/1213651\n",
      "2025-06-24 10:04:04,163 [INFO] Processati 590000/1213651\n",
      "2025-06-24 10:04:05,508 [INFO] Processati 595000/1213651\n",
      "2025-06-24 10:04:06,899 [INFO] Processati 600000/1213651\n",
      "2025-06-24 10:04:08,285 [INFO] Processati 605000/1213651\n",
      "2025-06-24 10:04:09,657 [INFO] Processati 610000/1213651\n",
      "2025-06-24 10:04:11,127 [INFO] Processati 615000/1213651\n",
      "2025-06-24 10:04:12,489 [INFO] Processati 620000/1213651\n",
      "2025-06-24 10:04:13,844 [INFO] Processati 625000/1213651\n",
      "2025-06-24 10:04:15,014 [INFO] Processati 630000/1213651\n",
      "2025-06-24 10:04:16,190 [INFO] Processati 635000/1213651\n",
      "2025-06-24 10:04:17,322 [INFO] Processati 640000/1213651\n",
      "2025-06-24 10:04:18,642 [INFO] Processati 645000/1213651\n",
      "2025-06-24 10:04:20,084 [INFO] Processati 650000/1213651\n",
      "2025-06-24 10:04:21,283 [INFO] Processati 655000/1213651\n",
      "2025-06-24 10:04:22,643 [INFO] Processati 660000/1213651\n",
      "2025-06-24 10:04:23,850 [INFO] Processati 665000/1213651\n",
      "2025-06-24 10:04:25,254 [INFO] Processati 670000/1213651\n",
      "2025-06-24 10:04:26,634 [INFO] Processati 675000/1213651\n",
      "2025-06-24 10:04:28,045 [INFO] Processati 680000/1213651\n",
      "2025-06-24 10:04:29,494 [INFO] Processati 685000/1213651\n",
      "2025-06-24 10:04:30,948 [INFO] Processati 690000/1213651\n",
      "2025-06-24 10:04:32,358 [INFO] Processati 695000/1213651\n",
      "2025-06-24 10:04:33,771 [INFO] Processati 700000/1213651\n",
      "2025-06-24 10:04:35,203 [INFO] Processati 705000/1213651\n",
      "2025-06-24 10:04:36,591 [INFO] Processati 710000/1213651\n",
      "2025-06-24 10:04:38,062 [INFO] Processati 715000/1213651\n",
      "2025-06-24 10:04:39,244 [INFO] Processati 720000/1213651\n",
      "2025-06-24 10:04:40,397 [INFO] Processati 725000/1213651\n",
      "2025-06-24 10:04:41,518 [INFO] Processati 730000/1213651\n",
      "2025-06-24 10:04:42,610 [INFO] Processati 735000/1213651\n",
      "2025-06-24 10:04:43,680 [INFO] Processati 740000/1213651\n",
      "2025-06-24 10:04:44,814 [INFO] Processati 745000/1213651\n",
      "2025-06-24 10:04:46,001 [INFO] Processati 750000/1213651\n",
      "2025-06-24 10:04:47,230 [INFO] Processati 755000/1213651\n",
      "2025-06-24 10:04:48,421 [INFO] Processati 760000/1213651\n",
      "2025-06-24 10:04:49,718 [INFO] Processati 765000/1213651\n",
      "2025-06-24 10:04:50,956 [INFO] Processati 770000/1213651\n",
      "2025-06-24 10:04:52,439 [INFO] Processati 775000/1213651\n",
      "2025-06-24 10:04:54,110 [INFO] Processati 780000/1213651\n",
      "2025-06-24 10:04:55,608 [INFO] Processati 785000/1213651\n",
      "2025-06-24 10:04:57,140 [INFO] Processati 790000/1213651\n",
      "2025-06-24 10:04:58,776 [INFO] Processati 795000/1213651\n",
      "2025-06-24 10:05:00,224 [INFO] Processati 800000/1213651\n",
      "2025-06-24 10:05:01,824 [INFO] Processati 805000/1213651\n",
      "2025-06-24 10:05:03,450 [INFO] Processati 810000/1213651\n",
      "2025-06-24 10:05:05,091 [INFO] Processati 815000/1213651\n",
      "2025-06-24 10:05:06,637 [INFO] Processati 820000/1213651\n",
      "2025-06-24 10:05:08,150 [INFO] Processati 825000/1213651\n",
      "2025-06-24 10:05:09,694 [INFO] Processati 830000/1213651\n",
      "2025-06-24 10:05:11,179 [INFO] Processati 835000/1213651\n",
      "2025-06-24 10:05:12,698 [INFO] Processati 840000/1213651\n",
      "2025-06-24 10:05:14,270 [INFO] Processati 845000/1213651\n",
      "2025-06-24 10:05:15,755 [INFO] Processati 850000/1213651\n",
      "2025-06-24 10:05:17,166 [INFO] Processati 855000/1213651\n",
      "2025-06-24 10:05:18,678 [INFO] Processati 860000/1213651\n",
      "2025-06-24 10:05:20,130 [INFO] Processati 865000/1213651\n",
      "2025-06-24 10:05:21,551 [INFO] Processati 870000/1213651\n",
      "2025-06-24 10:05:23,020 [INFO] Processati 875000/1213651\n",
      "2025-06-24 10:05:24,493 [INFO] Processati 880000/1213651\n",
      "2025-06-24 10:05:25,958 [INFO] Processati 885000/1213651\n",
      "2025-06-24 10:05:27,432 [INFO] Processati 890000/1213651\n",
      "2025-06-24 10:05:28,867 [INFO] Processati 895000/1213651\n",
      "2025-06-24 10:05:30,417 [INFO] Processati 900000/1213651\n",
      "2025-06-24 10:05:32,025 [INFO] Processati 905000/1213651\n",
      "2025-06-24 10:05:33,602 [INFO] Processati 910000/1213651\n",
      "2025-06-24 10:05:35,264 [INFO] Processati 915000/1213651\n",
      "2025-06-24 10:05:36,800 [INFO] Processati 920000/1213651\n",
      "2025-06-24 10:05:38,325 [INFO] Processati 925000/1213651\n",
      "2025-06-24 10:05:39,844 [INFO] Processati 930000/1213651\n",
      "2025-06-24 10:05:41,352 [INFO] Processati 935000/1213651\n",
      "2025-06-24 10:05:42,885 [INFO] Processati 940000/1213651\n",
      "2025-06-24 10:05:44,389 [INFO] Processati 945000/1213651\n",
      "2025-06-24 10:05:45,893 [INFO] Processati 950000/1213651\n",
      "2025-06-24 10:05:47,414 [INFO] Processati 955000/1213651\n",
      "2025-06-24 10:05:48,960 [INFO] Processati 960000/1213651\n",
      "2025-06-24 10:05:50,339 [INFO] Processati 965000/1213651\n",
      "2025-06-24 10:05:51,726 [INFO] Processati 970000/1213651\n",
      "2025-06-24 10:05:53,120 [INFO] Processati 975000/1213651\n",
      "2025-06-24 10:05:54,525 [INFO] Processati 980000/1213651\n",
      "2025-06-24 10:05:55,946 [INFO] Processati 985000/1213651\n",
      "2025-06-24 10:05:57,352 [INFO] Processati 990000/1213651\n",
      "2025-06-24 10:05:58,762 [INFO] Processati 995000/1213651\n",
      "2025-06-24 10:06:00,222 [INFO] Processati 1000000/1213651\n",
      "2025-06-24 10:06:01,773 [INFO] Processati 1005000/1213651\n",
      "2025-06-24 10:06:03,178 [INFO] Processati 1010000/1213651\n",
      "2025-06-24 10:06:04,627 [INFO] Processati 1015000/1213651\n",
      "2025-06-24 10:06:06,062 [INFO] Processati 1020000/1213651\n",
      "2025-06-24 10:06:07,462 [INFO] Processati 1025000/1213651\n",
      "2025-06-24 10:06:08,932 [INFO] Processati 1030000/1213651\n",
      "2025-06-24 10:06:10,384 [INFO] Processati 1035000/1213651\n",
      "2025-06-24 10:06:11,800 [INFO] Processati 1040000/1213651\n",
      "2025-06-24 10:06:13,237 [INFO] Processati 1045000/1213651\n",
      "2025-06-24 10:06:14,666 [INFO] Processati 1050000/1213651\n",
      "2025-06-24 10:06:16,107 [INFO] Processati 1055000/1213651\n",
      "2025-06-24 10:06:17,564 [INFO] Processati 1060000/1213651\n",
      "2025-06-24 10:06:18,978 [INFO] Processati 1065000/1213651\n",
      "2025-06-24 10:06:20,355 [INFO] Processati 1070000/1213651\n",
      "2025-06-24 10:06:21,706 [INFO] Processati 1075000/1213651\n",
      "2025-06-24 10:06:23,056 [INFO] Processati 1080000/1213651\n",
      "2025-06-24 10:06:24,324 [INFO] Processati 1085000/1213651\n",
      "2025-06-24 10:06:25,618 [INFO] Processati 1090000/1213651\n",
      "2025-06-24 10:06:26,966 [INFO] Processati 1095000/1213651\n",
      "2025-06-24 10:06:28,320 [INFO] Processati 1100000/1213651\n",
      "2025-06-24 10:06:29,667 [INFO] Processati 1105000/1213651\n",
      "2025-06-24 10:06:31,013 [INFO] Processati 1110000/1213651\n",
      "2025-06-24 10:06:32,895 [INFO] Processati 1115000/1213651\n",
      "2025-06-24 10:06:34,498 [INFO] Processati 1120000/1213651\n",
      "2025-06-24 10:06:35,992 [INFO] Processati 1125000/1213651\n",
      "2025-06-24 10:06:37,438 [INFO] Processati 1130000/1213651\n",
      "2025-06-24 10:06:38,934 [INFO] Processati 1135000/1213651\n",
      "2025-06-24 10:06:40,316 [INFO] Processati 1140000/1213651\n",
      "2025-06-24 10:06:41,756 [INFO] Processati 1145000/1213651\n",
      "2025-06-24 10:06:43,128 [INFO] Processati 1150000/1213651\n",
      "2025-06-24 10:06:44,527 [INFO] Processati 1155000/1213651\n",
      "2025-06-24 10:06:45,943 [INFO] Processati 1160000/1213651\n",
      "2025-06-24 10:06:47,377 [INFO] Processati 1165000/1213651\n",
      "2025-06-24 10:06:48,870 [INFO] Processati 1170000/1213651\n",
      "2025-06-24 10:06:50,311 [INFO] Processati 1175000/1213651\n",
      "2025-06-24 10:06:51,837 [INFO] Processati 1180000/1213651\n",
      "2025-06-24 10:06:53,720 [INFO] Processati 1185000/1213651\n",
      "2025-06-24 10:06:55,048 [INFO] Processati 1190000/1213651\n",
      "2025-06-24 10:06:56,349 [INFO] Processati 1195000/1213651\n",
      "2025-06-24 10:06:57,647 [INFO] Processati 1200000/1213651\n",
      "2025-06-24 10:06:58,982 [INFO] Processati 1205000/1213651\n",
      "2025-06-24 10:07:00,402 [INFO] Processati 1210000/1213651\n",
      "2025-06-24 10:17:21,300 [INFO] Processed SMILES: 1201609/1203953\n",
      "2025-06-24 10:17:21,300 [INFO] Unique tokens: 36\n",
      "2025-06-24 10:17:21,301 [INFO] Max length: 94\n",
      "2025-06-24 10:17:21,304 [INFO] Vocabolario creato:\n",
      "2025-06-24 10:17:21,304 [INFO] ['<PAD>', '<START>', '<END>', '#', '%', '(', ')', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '=', 'Br', 'C', 'Cl', 'F', 'I', 'N', 'O', 'P', 'S', '[C+]', '[C-]', '[CH-]', '[I+]', '[N+]', '[N-]', '[O+]', '[O-]', '[P+]', '[PH]', '[S+]', '[S]']\n",
      "2025-06-24 10:17:21,306 [INFO] Mappature e vocabolario salvati su disco.\n",
      "2025-06-24 10:17:21,558 [WARNING] Stratificazione disabilitata: alcune classi hanno meno di 2 esempi.\n"
     ]
    }
   ],
   "source": [
    "# Configurazione del logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Funzione per calcolare la complessit√† di una SMILES ---\n",
    "def compute_complexity_from_tokens(tokens: List[str]) -> int:\n",
    "    \"\"\"\n",
    "    Calcola la complessit√† di una SMILES come la somma del numero di anelli (ottenuti con GetSSSR)\n",
    "    e del numero di ramificazioni (conteggio delle parentesi aperte).\n",
    "    \"\"\"\n",
    "    smiles = ''.join(tokens)\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if not mol:\n",
    "            return len(tokens)  # fallback: usa la lunghezza come complessit√†\n",
    "        num_rings = len(Chem.GetSSSR(mol))  # Correzione: ottiene la lunghezza dell'iterabile\n",
    "        num_branches = smiles.count('(')\n",
    "        return num_rings + num_branches\n",
    "    except Exception:\n",
    "        return len(tokens)  # fallback: usa la lunghezza come complessit√†\n",
    "\n",
    "\n",
    "# --- Funzioni di Preprocessing ---\n",
    "def randomize_smiles(smiles: str, num_versions: int = 3) -> List[str]:\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if not mol:\n",
    "        return []\n",
    "    randomized = []\n",
    "    for _ in range(num_versions):\n",
    "        try:\n",
    "            new_smiles = Chem.MolToSmiles(mol, doRandom=True, canonical=False)\n",
    "            if new_smiles:\n",
    "                randomized.append(new_smiles)\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Randomization error: {e}\")\n",
    "            continue\n",
    "    return randomized\n",
    "\n",
    "def validate_and_fix_smiles(smiles: str) -> str:\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles, sanitize=True)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        try:\n",
    "            Chem.Kekulize(mol, clearAromaticFlags=True)\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Kekulization error in SMILES {smiles}: {e}\")\n",
    "            return None\n",
    "        return Chem.MolToSmiles(mol, canonical=True, isomericSmiles=False)\n",
    "    except Exception as e:\n",
    "        logger.debug(f\"Parsing/sanitization error in SMILES {smiles}: {e}\")\n",
    "        return None\n",
    "\n",
    "def robust_tokenize(smiles: str) -> list:\n",
    "    pattern = (\n",
    "        r\"(\\[[^\\[\\]]{1,6}\\]|\"                 # atomi in parentesi quadre\n",
    "        r\"Br|Cl|Si|Na|Mg|Mn|Ca|Fe|Zn|Se|Li|K|Al|B|\"  # elementi multi-char\n",
    "        r\"R[0-9]|r[0-9]|a[0-9]|\"             # ring labels\n",
    "        r\"[A-Za-z0-9@+\\-\\\\\\/\\(\\)=#\\$\\.\\%,])\"  # singoli caratteri, incluso '%'\n",
    "    )\n",
    "    tokens = re.findall(pattern, smiles)\n",
    "    tokens = re.findall(pattern, smiles)\n",
    "    stack = []\n",
    "    for t in tokens:\n",
    "        if t.startswith('['):\n",
    "            stack.append(t)\n",
    "        if t.endswith(']') and not stack:\n",
    "            return []\n",
    "        if t.endswith(']'):\n",
    "            stack.pop()\n",
    "    try:\n",
    "        if not stack and Chem.MolFromSmiles(''.join(tokens)):\n",
    "            return tokens\n",
    "    except Exception as e:\n",
    "        logger.debug(f\"Tokenization error: {e}\")\n",
    "        return []\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def process_dataset(data: List[str]) -> Tuple[List[List[str]], List[str], int]:\n",
    "    processed = []\n",
    "    all_tokens = set()\n",
    "    for s in data:\n",
    "        fixed = validate_and_fix_smiles(s)\n",
    "        if not fixed:\n",
    "            continue\n",
    "        tokens = robust_tokenize(fixed)\n",
    "        if tokens and 3 <= len(tokens) <= config.MAX_LENGTH - 2:\n",
    "            processed.append(tokens)\n",
    "            all_tokens.update(tokens)\n",
    "    vocab = ['<PAD>', '<START>', '<END>'] + sorted(all_tokens)\n",
    "    lengths = [len(t) for t in processed]\n",
    "    max_len = min(int(np.percentile(lengths, 99)) + 2, config.MAX_LENGTH) if processed else config.MAX_LENGTH\n",
    "    logger.info(f\"Processed SMILES: {len(processed)}/{len(data)}\")\n",
    "    logger.info(f\"Unique tokens: {len(all_tokens)}\")\n",
    "    logger.info(f\"Max length: {max_len}\")\n",
    "    return processed, vocab, max_len\n",
    "\n",
    "# --- Componenti del Modello ---\n",
    "class DynamicPositionalEncoding(Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "    def build(self, input_shape):\n",
    "        max_seq_len = input_shape[1]\n",
    "        pos = np.arange(max_seq_len)[:, np.newaxis]\n",
    "        i = np.arange(self.embed_dim)[np.newaxis, :]\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(self.embed_dim))\n",
    "        angle_rads = pos * angle_rates\n",
    "        angle_rads[:, 0::2] = tf.math.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = tf.math.cos(angle_rads[:, 1::2])\n",
    "        self.pos_encoding = tf.cast(angle_rads[np.newaxis, ...], dtype=tf.float32)\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        base_config.update({\"embed_dim\": self.embed_dim})\n",
    "        return base_config\n",
    "\n",
    "class ImprovedTransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim, rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.rate = rate\n",
    "        self.mha = MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embed_dim,\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(config.L2_REG),\n",
    "            dropout=rate\n",
    "        )\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ffn_dim, activation=\"gelu\", kernel_regularizer=tf.keras.regularizers.l2(config.L2_REG)),\n",
    "            Dense(embed_dim, kernel_regularizer=tf.keras.regularizers.l2(config.L2_REG))\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        causal_mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        attn_output = self.mha(inputs, inputs, attention_mask=causal_mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        base_config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"ffn_dim\": self.ffn_dim,\n",
    "            \"rate\": self.rate\n",
    "        })\n",
    "        return base_config\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, embed_dim, warmup_steps=10000):\n",
    "        super().__init__()\n",
    "        self.embed_dim = tf.cast(embed_dim, tf.float32)\n",
    "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32) + 1e-9\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.embed_dim) * tf.math.minimum(arg1, arg2)\n",
    "    def get_config(self):\n",
    "        return {\"embed_dim\": self.embed_dim.numpy(), \"warmup_steps\": self.warmup_steps.numpy()}\n",
    "\n",
    "def build_improved_model(vocab_size: int) -> Model:\n",
    "    inputs = Input(shape=(config.MAX_LENGTH,))\n",
    "    x = Embedding(vocab_size, config.EMBED_DIM, mask_zero=True)(inputs)\n",
    "    x = DynamicPositionalEncoding(config.EMBED_DIM)(x)\n",
    "    x = Dropout(config.DROPOUT_RATE)(x)\n",
    "    for _ in range(config.TRANSFORMER_LAYERS):\n",
    "        x = ImprovedTransformerBlock(config.EMBED_DIM, config.TRANSFORMER_HEADS, config.FF_DIM, rate=config.DROPOUT_RATE)(x)\n",
    "    outputs = Dense(vocab_size)(x)\n",
    "    def smoothed_loss(y_true, y_pred):\n",
    "        # cast a interi perch√© tf.nn.sparse_* vuole etichette int32 o int64\n",
    "        y_true_int = tf.cast(y_true, tf.int32)\n",
    "        mask = tf.cast(tf.math.not_equal(y_true, 0), tf.float32)\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true_int, logits=y_pred)\n",
    "        return tf.reduce_sum(loss * mask) / (tf.reduce_sum(mask) + 1e-9)\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=CustomSchedule(config.EMBED_DIM),\n",
    "        clipnorm=config.GRADIENT_CLIP\n",
    "    )\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=optimizer, loss=smoothed_loss)\n",
    "    return model\n",
    "\n",
    "# --- Generatori Dati e Utilities ---\n",
    "class ThreadSafeIterator:\n",
    "    def __init__(self, iterator):\n",
    "        self.iterator = iterator\n",
    "        self.lock = Lock()\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        with self.lock:\n",
    "            return next(self.iterator)\n",
    "\n",
    "def threadsafe_generator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        return ThreadSafeIterator(func(*args, **kwargs))\n",
    "    return wrapper\n",
    "\n",
    "class CurriculumSmilesGenerator:\n",
    "    \"\"\"\n",
    "    Versione semplificata e robusta del generatore con curriculum learning\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenized_smiles: List[List[str]], vocab: List[str]):\n",
    "        self.char2idx = {c: i for i, c in enumerate(vocab)}\n",
    "        self.idx2char = {i: c for c, i in self.char2idx.items()}\n",
    "        \n",
    "        # Calcola la complessit√† di ogni SMILES\n",
    "        logger.info(\"Calcolo complessit√† delle molecole...\")\n",
    "        self.data_with_complexity = []\n",
    "        complexities = []\n",
    "        \n",
    "        for tokens in tokenized_smiles:\n",
    "            complexity = compute_complexity_from_tokens(tokens)\n",
    "            self.data_with_complexity.append((tokens, complexity))\n",
    "            complexities.append(complexity)\n",
    "        \n",
    "        # Calcola statistiche\n",
    "        self.min_complexity = min(complexities) if complexities else 0\n",
    "        self.max_complexity = max(complexities) if complexities else 0\n",
    "        \n",
    "        # Inizializza al minimo\n",
    "        self.current_complexity = config.CURRICULUM_START_COMPLEXITY\n",
    "        self.train_smiles = {''.join(tokens) for tokens, _ in self.data_with_complexity}\n",
    "        \n",
    "        # Log iniziali\n",
    "        logger.info(f\"Curriculum inizializzato: min={self.min_complexity}, max={self.max_complexity}\")\n",
    "        logger.info(f\"Complessit√† iniziale: {self.current_complexity}\")\n",
    "        \n",
    "        # Crea indici per accesso rapido\n",
    "        self._prepare_complexity_indices()\n",
    "    \n",
    "    def _prepare_complexity_indices(self):\n",
    "        \"\"\"Prepara gli indici per accesso rapido per complessit√†\"\"\"\n",
    "        self.complexity_indices = {}\n",
    "        for i, (_, comp) in enumerate(self.data_with_complexity):\n",
    "            if comp not in self.complexity_indices:\n",
    "                self.complexity_indices[comp] = []\n",
    "            self.complexity_indices[comp].append(i)\n",
    "        \n",
    "        # Prepara gli indici disponibili\n",
    "        self.available_indices = []\n",
    "        for comp in range(self.min_complexity, self.current_complexity + 1):\n",
    "            if comp in self.complexity_indices:\n",
    "                self.available_indices.extend(self.complexity_indices[comp])\n",
    "    \n",
    "    def update_complexity(self, epoch, loss_diff=None):\n",
    "        \"\"\"Aggiorna la complessit√† in base all'epoca o alla convergenza\"\"\"\n",
    "        old_complexity = self.current_complexity\n",
    "        \n",
    "        if epoch < config.WARMUP_EPOCHS:\n",
    "            # Durante il warmup, aumenta linearmente\n",
    "            progress = (epoch + 1) / config.WARMUP_EPOCHS\n",
    "            target = self.min_complexity + progress * (self.max_complexity - self.min_complexity)\n",
    "            new_complexity = round(target)\n",
    "            self.current_complexity = max(min(new_complexity, self.max_complexity), self.current_complexity)\n",
    "            reason = f\"warmup ({epoch+1}/{config.WARMUP_EPOCHS})\"\n",
    "        elif loss_diff is not None and loss_diff < config.LOSS_STABILITY_THRESHOLD:\n",
    "            # Dopo il warmup, aumenta se la loss √® stabile\n",
    "            new_complexity = min(self.current_complexity + config.CURRICULUM_COMPLEXITY_STEP, self.max_complexity)\n",
    "            if new_complexity > self.current_complexity:\n",
    "                self.current_complexity = new_complexity\n",
    "                reason = f\"convergenza (diff={loss_diff:.5f})\"\n",
    "            else:\n",
    "                reason = \"gi√† alla massima complessit√†\"\n",
    "        else:\n",
    "            reason = \"nessun cambiamento\"\n",
    "        \n",
    "        # Aggiorna gli indici disponibili\n",
    "        self.available_indices = []\n",
    "        for comp in range(self.min_complexity, self.current_complexity + 1):\n",
    "            if comp in self.complexity_indices:\n",
    "                self.available_indices.extend(self.complexity_indices[comp])\n",
    "        \n",
    "        # Log\n",
    "        if old_complexity != self.current_complexity:\n",
    "            available_percent = len(self.available_indices) / len(self.data_with_complexity)\n",
    "            logger.info(f\"Curriculum: complessit√† {old_complexity} -> {self.current_complexity} ({reason})\")\n",
    "            logger.info(f\"Esempi disponibili: {len(self.available_indices)}/{len(self.data_with_complexity)} ({available_percent:.1%})\")\n",
    "        elif epoch % 5 == 0:\n",
    "            available_percent = len(self.available_indices) / len(self.data_with_complexity)\n",
    "            logger.info(f\"[Epoca {epoch}] Curriculum status: complessit√†={self.current_complexity}/{self.max_complexity}, \"\n",
    "                       f\"esempi={len(self.available_indices)}/{len(self.data_with_complexity)} ({available_percent:.1%})\")\n",
    "    \n",
    "    def __call__(self):\n",
    "        \"\"\"Generatore di batch per l'addestramento\"\"\"\n",
    "        while True:\n",
    "            inputs = np.full((config.BATCH_SIZE, config.MAX_LENGTH), self.char2idx['<PAD>'], dtype=np.int32)\n",
    "            targets = np.full_like(inputs, self.char2idx['<PAD>'])\n",
    "            \n",
    "            # Verifica che ci siano indici disponibili\n",
    "            if not self.available_indices:\n",
    "                logger.warning(f\"Nessun esempio disponibile alla complessit√† {self.current_complexity}!\")\n",
    "                indices = list(range(len(self.data_with_complexity)))\n",
    "            else:\n",
    "                indices = self.available_indices\n",
    "            \n",
    "            for i in range(config.BATCH_SIZE):\n",
    "                # Seleziona un esempio casuale\n",
    "                idx = random.choice(indices)\n",
    "                tokens = self.data_with_complexity[idx][0]\n",
    "                \n",
    "                # Data augmentation (randomizzazione)\n",
    "                if random.random() < config.AUGMENT_PROB:\n",
    "                    try:\n",
    "                        smiles = ''.join(tokens)\n",
    "                        mol = Chem.MolFromSmiles(smiles)\n",
    "                        if mol:\n",
    "                            new_smiles = Chem.MolToSmiles(mol, doRandom=True, canonical=False)\n",
    "                            if new_smiles:\n",
    "                                new_tokens = robust_tokenize(new_smiles)\n",
    "                                if new_tokens:\n",
    "                                    tokens = new_tokens\n",
    "                    except Exception:\n",
    "                        pass  # In caso di errore, usa i token originali\n",
    "                \n",
    "                # Prepara la sequenza\n",
    "                seq = ['<START>'] + tokens + ['<END>']\n",
    "                padded = (seq + ['<PAD>'] * config.MAX_LENGTH)[:config.MAX_LENGTH]\n",
    "                \n",
    "                # Converti in indici\n",
    "                inputs[i] = [self.char2idx.get(t, self.char2idx['<PAD>']) for t in padded]\n",
    "                targets[i, :-1] = inputs[i][1:]\n",
    "                targets[i, -1] = self.char2idx['<PAD>']\n",
    "            \n",
    "            yield inputs, targets\n",
    "    \n",
    "    def get_dataset(self):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            self.__call__,\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=(config.BATCH_SIZE, config.MAX_LENGTH), dtype=tf.int32),\n",
    "                tf.TensorSpec(shape=(config.BATCH_SIZE, config.MAX_LENGTH), dtype=tf.int32)\n",
    "            )\n",
    "        ).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "\n",
    "# --- Callback per il monitoraggio ---\n",
    "class CustomTensorBoard(tf.keras.callbacks.TensorBoard):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        lr = self.model.optimizer.learning_rate\n",
    "        if isinstance(lr, tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "            logs['lr'] = lr(epoch).numpy()\n",
    "        else:\n",
    "            logs['lr'] = lr.numpy()\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "\n",
    "class EnhancedTrainingMonitor(Callback):\n",
    "    def __init__(self, val_gen: CurriculumSmilesGenerator):\n",
    "        super().__init__()\n",
    "        self.val_gen = val_gen\n",
    "        self.best_val_loss = np.inf\n",
    "        self.prev_val_loss = None\n",
    "    def generate_smiles(self, num: int) -> Tuple[List[str], List[str]]:\n",
    "        generated, valid = [], []\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Il modello non √® stato assegnato al callback!\")\n",
    "        input_seq = np.full((1, config.MAX_LENGTH), self.val_gen.char2idx['<PAD>'], dtype=np.int32)\n",
    "        input_seq[0, 0] = self.val_gen.char2idx['<START>']\n",
    "        for _ in range(num):\n",
    "            for t in range(1, config.MAX_LENGTH):\n",
    "                logits = self.model(input_seq, training=False)[0, t-1]\n",
    "                probs = tf.nn.softmax(logits / config.TEMPERATURE).numpy()\n",
    "                valid_indices = [i for i, tok in self.val_gen.idx2char.items() \n",
    "                                 if re.match(r'^([A-Za-z0-9@#\\[\\]()+\\-\\\\/%=:.,]|<\\w+>)$', tok)]\n",
    "                probs[[i for i in range(len(probs)) if i not in valid_indices]] = 0\n",
    "                if np.sum(probs) == 0:\n",
    "                    break\n",
    "                probs /= np.sum(probs)\n",
    "                sampled = np.random.choice(len(probs), p=probs)\n",
    "                input_seq[0, t] = sampled\n",
    "                if sampled == self.val_gen.char2idx['<END>']:\n",
    "                    break\n",
    "            raw = ''.join([self.val_gen.idx2char[i] for i in input_seq[0]\n",
    "                           if i not in {self.val_gen.char2idx.get('<PAD>'), self.val_gen.char2idx['<END>']}][1:])\n",
    "            final = validate_and_fix_smiles(raw) or raw\n",
    "            generated.append(final)\n",
    "            if Chem.MolFromSmiles(final):\n",
    "                valid.append(final)\n",
    "            input_seq = np.full((1, config.MAX_LENGTH), self.val_gen.char2idx['<PAD>'], dtype=np.int32)\n",
    "            input_seq[0, 0] = self.val_gen.char2idx['<START>']\n",
    "        return generated, valid\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % config.PRINT_EVERY == 0:\n",
    "            generated, valid = self.generate_smiles(config.GEN_NUM)\n",
    "            validity = len(valid) / config.GEN_NUM\n",
    "            unique = len(set(valid))\n",
    "            novel = len([s for s in valid if s not in self.val_gen.train_smiles])\n",
    "            logger.info(f\"\\nüß™ Epoca {epoch+1}:\")\n",
    "            logger.info(f\"Loss Training: {logs.get('loss', 'N/A'):.4f} - Loss Validation: {logs.get('val_loss', 'N/A'):.4f}\")\n",
    "            logger.info(f\"Validit√†: {validity:.1%}\")\n",
    "            logger.info(f\"Unicit√†: {unique}/{len(valid)}\")\n",
    "            logger.info(f\"Novit√†: {novel}/{len(valid)}\")\n",
    "            if valid:\n",
    "                logger.info(\"Esempi:\")\n",
    "                for s in valid[:3]:\n",
    "                    logger.info(f\"- {s}\")\n",
    "            with open(\"valid_generated_smiles.txt\", \"w\") as f:\n",
    "                for s in valid:\n",
    "                    f.write(s + \"\\n\")\n",
    "\n",
    "# --- Main ---\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"üöÄ Avvio Training Transformer per SMILES con tokenizzazione BPE, data augmentation e curriculum learning basato sulla complessit√†\")\n",
    "    with open(config.SMILES_FILE) as f:\n",
    "        raw_smiles = [line.strip() for line in f if line.strip()]\n",
    "    logger.info(\"üîç Validazione SMILES...\")\n",
    "    valid_smiles = []\n",
    "    for idx, s in enumerate(raw_smiles):\n",
    "        if idx % 5000 == 0:\n",
    "            logger.info(f\"Processati {idx}/{len(raw_smiles)}\")\n",
    "        fixed = validate_and_fix_smiles(s)\n",
    "        if fixed and 3 <= len(fixed) <= config.MAX_LENGTH:\n",
    "            valid_smiles.append(fixed)\n",
    "    processed, vocab, max_len = process_dataset(valid_smiles)\n",
    "    config.MAX_LENGTH = max_len\n",
    "    logger.info(\"Vocabolario creato:\")\n",
    "    logger.info(vocab)\n",
    "    \n",
    "    import pickle\n",
    "    with open('char2idx.pkl', 'wb') as f:\n",
    "       pickle.dump({char: idx for idx, char in enumerate(vocab)}, f)\n",
    "    with open('idx2char.pkl', 'wb') as f:\n",
    "       pickle.dump({idx: char for idx, char in enumerate(vocab)}, f)\n",
    "    import json\n",
    "    with open(\"vocab.json\", \"w\") as f:\n",
    "        json.dump(vocab, f)\n",
    "    logger.info(\"Mappature e vocabolario salvati su disco.\")\n",
    "    \n",
    "    stratify_labels = [min(len(t), 20) for t in processed]\n",
    "    try:\n",
    "        counts = np.bincount(stratify_labels)\n",
    "        if np.min(counts[np.nonzero(counts)]) < 2:\n",
    "            logger.warning(\"Stratificazione disabilitata: alcune classi hanno meno di 2 esempi.\")\n",
    "            stratify_param = None\n",
    "        else:\n",
    "            stratify_param = stratify_labels\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Errore nella stratificazione: {e}. Disabilito stratificazione.\")\n",
    "        stratify_param = None\n",
    "\n",
    "    train_data, val_data = train_test_split(\n",
    "        processed,\n",
    "        test_size=config.VALID_RATIO,\n",
    "        stratify=stratify_param,\n",
    "        random_state=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6fdfed7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750753042.022811 3063298 cuda_executor.cc:479] failed to allocate 1.55GiB (1661075456 bytes) from device: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/keras/src/layers/layer.py:939: UserWarning: Layer 'dynamic_positional_encoding' (of type DynamicPositionalEncoding) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "2025-06-24 10:17:23,405 [INFO] Calcolo complessit√† delle molecole...\n",
      "2025-06-24 10:20:45,233 [INFO] Curriculum inizializzato: min=0, max=25\n",
      "2025-06-24 10:20:45,234 [INFO] Complessit√† iniziale: 5\n",
      "2025-06-24 10:20:45,524 [INFO] Calcolo complessit√† delle molecole...\n",
      "2025-06-24 10:21:08,483 [INFO] Curriculum inizializzato: min=0, max=22\n",
      "2025-06-24 10:21:08,484 [INFO] Complessit√† iniziale: 5\n",
      "2025-06-24 10:21:08,522 [INFO] [Epoca 0] Curriculum status: complessit√†=5/25, esempi=153839/1081448 (14.2%)\n",
      "2025-06-24 10:21:08,523 [INFO] [Epoca 0] Curriculum status: complessit√†=5/22, esempi=17029/120161 (14.2%)\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1750753277.327925 3063666 service.cc:152] XLA service 0x7c5a34002f90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1750753277.327944 3063666 service.cc:160]   StreamExecutor device (0): NVIDIA RTX A2000 12GB, Compute Capability 8.6\n",
      "2025-06-24 10:21:17.709854: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1750753279.827948 3063666 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-06-24 10:21:22.734749: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_103', 472 bytes spill stores, 472 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:23.096427: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_103', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:23.549111: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_103', 36 bytes spill stores, 36 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:23.604791: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_103', 1596 bytes spill stores, 1360 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:23.625676: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26', 352 bytes spill stores, 352 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:23.844573: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_103', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:24.061079: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26', 800 bytes spill stores, 612 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:24.444522: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_103', 628 bytes spill stores, 628 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:24.656291: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26', 36 bytes spill stores, 36 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:24.786814: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_103', 788 bytes spill stores, 788 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:24.817858: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26', 140 bytes spill stores, 140 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:25.008691: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26', 916 bytes spill stores, 744 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:25.260116: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_28', 916 bytes spill stores, 744 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:25.381822: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:25.531635: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_103', 552 bytes spill stores, 552 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:25.722086: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_103', 144 bytes spill stores, 144 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:26.239690: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26', 452 bytes spill stores, 352 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:26.311703: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_28', 800 bytes spill stores, 612 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:26.811670: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_113', 232 bytes spill stores, 232 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:27.048457: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_28', 140 bytes spill stores, 140 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:27.341475: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26', 1040 bytes spill stores, 1108 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:27.436354: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_113', 716 bytes spill stores, 720 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:27.852029: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_28', 140 bytes spill stores, 140 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:27.997817: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_113', 416 bytes spill stores, 416 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:28.265085: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26', 140 bytes spill stores, 140 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:28.277122: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_28', 1040 bytes spill stores, 1108 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:28.287804: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_28', 268 bytes spill stores, 268 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:28.641347: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_28', 352 bytes spill stores, 352 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:28.858376: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_30', 408 bytes spill stores, 408 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:28.982147: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_28', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:29.012306: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_113', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:29.017391: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_113', 336 bytes spill stores, 336 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:29.302103: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_28', 36 bytes spill stores, 36 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:29.575539: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_30', 148 bytes spill stores, 148 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:29.594357: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_26', 268 bytes spill stores, 268 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:30.129994: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_28', 452 bytes spill stores, 352 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:30.582606: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_114', 628 bytes spill stores, 628 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:30.649850: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_30', 376 bytes spill stores, 376 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:30.655669: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_113', 416 bytes spill stores, 416 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:31.510411: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_113', 148 bytes spill stores, 148 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:31.767038: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_30', 840 bytes spill stores, 840 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:31.866016: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_30_0', 388 bytes spill stores, 388 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:31.898360: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_114', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:32.744781: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9683', 188 bytes spill stores, 188 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:32.771688: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_114', 784 bytes spill stores, 784 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:33.146261: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_113', 1712 bytes spill stores, 1476 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:33.247769: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_30', 268 bytes spill stores, 268 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:33.699432: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9526', 500 bytes spill stores, 500 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:33.911380: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_114', 552 bytes spill stores, 552 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:34.100577: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_30', 740 bytes spill stores, 740 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:34.173476: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_114', 472 bytes spill stores, 472 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:34.321306: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9683', 760 bytes spill stores, 760 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:34.344437: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_30', 444 bytes spill stores, 444 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:34.678808: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_114', 144 bytes spill stores, 144 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:35.593500: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_114', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:35.843889: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9683', 908 bytes spill stores, 908 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:36.059697: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_114', 1576 bytes spill stores, 1340 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:36.623355: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_30_0', 1328 bytes spill stores, 1168 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:36.710732: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9526', 760 bytes spill stores, 760 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:37.708150: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9526', 188 bytes spill stores, 188 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:37.833268: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_114', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:37.910899: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9526', 1828 bytes spill stores, 1796 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:38.104725: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_30', 148 bytes spill stores, 148 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:39.195260: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9683', 500 bytes spill stores, 500 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:39.231596: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_30', 4336 bytes spill stores, 3668 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:39.323228: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9526', 636 bytes spill stores, 636 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:39.830437: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9526', 424 bytes spill stores, 424 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:39.986056: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9526', 908 bytes spill stores, 908 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:40.135498: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9526', 268 bytes spill stores, 268 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:40.324646: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_30', 496 bytes spill stores, 496 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:40.348306: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9683', 268 bytes spill stores, 268 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:40.414203: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9683', 424 bytes spill stores, 424 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:40.780997: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9683', 636 bytes spill stores, 636 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:41.051453: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9683', 496 bytes spill stores, 496 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:41.108092: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9526', 496 bytes spill stores, 496 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:42.108030: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_9683', 1828 bytes spill stores, 1796 bytes spill loads\n",
      "\n",
      "2025-06-24 10:21:57.639454: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'fusion_1767', 152 bytes spill stores, 152 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'fusion_1765', 332 bytes spill stores, 332 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'fusion_1762', 332 bytes spill stores, 332 bytes spill loads\n",
      "\n",
      "I0000 00:00:1750753317.903293 3063666 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2025-06-24 10:29:48,511 [WARNING] You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16897/16897 - 520s - 31ms/step - loss: 1.0121 - val_loss: 0.7626 - learning_rate: 7.9347e-04 - lr: 7.9347e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 10:37:41,172 [WARNING] You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16897/16897 - 473s - 28ms/step - loss: 0.7792 - val_loss: 0.7296 - learning_rate: 5.6107e-04 - lr: 5.6107e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 10:37:41,378 [INFO] Curriculum: complessit√† 5 -> 8 (warmup (3/10))\n",
      "2025-06-24 10:37:41,379 [INFO] Esempi disponibili: 603447/1081448 (55.8%)\n",
      "2025-06-24 10:37:41,380 [INFO] Curriculum: complessit√† 5 -> 7 (warmup (3/10))\n",
      "2025-06-24 10:37:41,381 [INFO] Esempi disponibili: 49364/120161 (41.1%)\n",
      "2025-06-24 10:45:59,177 [WARNING] You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16897/16897 - 498s - 29ms/step - loss: 0.7561 - val_loss: 0.7141 - learning_rate: 4.5811e-04 - lr: 4.5811e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 10:45:59,360 [INFO] Curriculum: complessit√† 8 -> 10 (warmup (4/10))\n",
      "2025-06-24 10:45:59,361 [INFO] Esempi disponibili: 849896/1081448 (78.6%)\n",
      "2025-06-24 10:45:59,364 [INFO] Curriculum: complessit√† 7 -> 9 (warmup (4/10))\n",
      "2025-06-24 10:45:59,365 [INFO] Esempi disponibili: 82042/120161 (68.3%)\n",
      "2025-06-24 10:54:43,550 [WARNING] You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16897/16897 - 524s - 31ms/step - loss: 0.7441 - val_loss: 0.7002 - learning_rate: 3.9674e-04 - lr: 3.9674e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 10:54:43,715 [INFO] Curriculum: complessit√† 10 -> 12 (warmup (5/10))\n",
      "2025-06-24 10:54:43,716 [INFO] Esempi disponibili: 986766/1081448 (91.2%)\n",
      "2025-06-24 10:54:43,718 [INFO] Curriculum: complessit√† 9 -> 11 (warmup (5/10))\n",
      "2025-06-24 10:54:43,719 [INFO] Esempi disponibili: 103254/120161 (85.9%)\n",
      "2025-06-24 11:04:02,811 [WARNING] You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16897/16897 - 559s - 33ms/step - loss: 0.7400 - val_loss: 0.6920 - learning_rate: 3.5485e-04 - lr: 3.5485e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 11:04:02,996 [INFO] Curriculum: complessit√† 12 -> 15 (warmup (6/10))\n",
      "2025-06-24 11:04:02,997 [INFO] Esempi disponibili: 1062679/1081448 (98.3%)\n",
      "2025-06-24 11:04:02,999 [INFO] Curriculum: complessit√† 11 -> 13 (warmup (6/10))\n",
      "2025-06-24 11:04:03,000 [INFO] Esempi disponibili: 113705/120161 (94.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training plots at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 11:13:54,747 [WARNING] You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16897/16897 - 590s - 35ms/step - loss: 0.7405 - val_loss: 0.6917 - learning_rate: 3.2393e-04 - lr: 3.2393e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 11:13:54,915 [INFO] Curriculum: complessit√† 15 -> 18 (warmup (7/10))\n",
      "2025-06-24 11:13:54,916 [INFO] Esempi disponibili: 1079121/1081448 (99.8%)\n",
      "2025-06-24 11:13:54,918 [INFO] Curriculum: complessit√† 13 -> 15 (warmup (7/10))\n",
      "2025-06-24 11:13:54,919 [INFO] Esempi disponibili: 118073/120161 (98.3%)\n",
      "2025-06-24 11:22:12,880 [WARNING] You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16897/16897 - 498s - 29ms/step - loss: 0.7384 - val_loss: 0.6910 - learning_rate: 2.9990e-04 - lr: 2.9990e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 11:22:13,104 [INFO] Curriculum: complessit√† 18 -> 20 (warmup (8/10))\n",
      "2025-06-24 11:22:13,105 [INFO] Esempi disponibili: 1081133/1081448 (100.0%)\n",
      "2025-06-24 11:22:13,108 [INFO] Curriculum: complessit√† 15 -> 18 (warmup (8/10))\n",
      "2025-06-24 11:22:13,109 [INFO] Esempi disponibili: 119913/120161 (99.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16897/16897 - 529s - 31ms/step - loss: 0.7344 - val_loss: 0.6910 - learning_rate: 2.8053e-04 - lr: 2.8053e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 11:31:01,814 [INFO] Curriculum: complessit√† 20 -> 22 (warmup (9/10))\n",
      "2025-06-24 11:31:01,814 [INFO] Esempi disponibili: 1081440/1081448 (100.0%)\n",
      "2025-06-24 11:31:01,817 [INFO] Curriculum: complessit√† 18 -> 20 (warmup (9/10))\n",
      "2025-06-24 11:31:01,818 [INFO] Esempi disponibili: 120124/120161 (100.0%)\n",
      "2025-06-24 11:39:09,778 [WARNING] You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16897/16897 - 488s - 29ms/step - loss: 0.7314 - val_loss: 0.6880 - learning_rate: 2.6449e-04 - lr: 2.6449e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 11:39:09,970 [INFO] Curriculum: complessit√† 22 -> 25 (warmup (10/10))\n",
      "2025-06-24 11:39:09,971 [INFO] Esempi disponibili: 1081448/1081448 (100.0%)\n",
      "2025-06-24 11:39:09,974 [INFO] Curriculum: complessit√† 20 -> 22 (warmup (10/10))\n",
      "2025-06-24 11:39:09,974 [INFO] Esempi disponibili: 120161/120161 (100.0%)\n",
      "2025-06-24 11:47:42,111 [WARNING] You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16897/16897 - 512s - 30ms/step - loss: 0.7281 - val_loss: 0.6857 - learning_rate: 2.5092e-04 - lr: 2.5092e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 11:47:42,317 [INFO] [Epoca 10] Curriculum status: complessit√†=25/25, esempi=1081448/1081448 (100.0%)\n",
      "2025-06-24 11:47:42,320 [INFO] [Epoca 10] Curriculum status: complessit√†=22/22, esempi=120161/120161 (100.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training plots at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 11:56:19,587 [WARNING] You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16897/16897 - 516s - 31ms/step - loss: 0.7259 - val_loss: 0.6840 - learning_rate: 2.3924e-04 - lr: 2.3924e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 12:04:46,221 [WARNING] You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16897/16897 - 507s - 30ms/step - loss: 0.7237 - val_loss: 0.6818 - learning_rate: 2.2906e-04 - lr: 2.2906e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 12:12:19,339 [WARNING] You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16897/16897 - 453s - 27ms/step - loss: 0.7218 - val_loss: 0.6806 - learning_rate: 2.2007e-04 - lr: 2.2007e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 12:19:52,209 [WARNING] You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16897/16897 - 453s - 27ms/step - loss: 0.7203 - val_loss: 0.6790 - learning_rate: 2.1206e-04 - lr: 2.1206e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 12:27:24,776 [WARNING] You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16897/16897 - 452s - 27ms/step - loss: 0.7190 - val_loss: 0.6776 - learning_rate: 2.0487e-04 - lr: 2.0487e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 12:27:24,953 [INFO] [Epoca 15] Curriculum status: complessit√†=25/25, esempi=1081448/1081448 (100.0%)\n",
      "2025-06-24 12:27:24,955 [INFO] [Epoca 15] Curriculum status: complessit√†=22/22, esempi=120161/120161 (100.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training plots at epoch 15\n",
      "16897/16897 - 452s - 27ms/step - loss: 0.7178 - val_loss: 0.6779 - learning_rate: 1.9837e-04 - lr: 1.9837e-04\n",
      "16897/16897 - 453s - 27ms/step - loss: 0.7161 - val_loss: 0.6778 - learning_rate: 1.9245e-04 - lr: 1.9245e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 12:50:02,076 [WARNING] You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16897/16897 - 451s - 27ms/step - loss: 0.7157 - val_loss: 0.6753 - learning_rate: 1.8702e-04 - lr: 1.8702e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 12:57:34,220 [WARNING] You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16897/16897 - 452s - 27ms/step - loss: 0.7144 - val_loss: 0.6744 - learning_rate: 1.8203e-04 - lr: 1.8203e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 13:05:06,019 [WARNING] You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16897/16897 - 452s - 27ms/step - loss: 0.7138 - val_loss: 0.6731 - learning_rate: 1.7743e-04 - lr: 1.7743e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 13:05:06,222 [INFO] [Epoca 20] Curriculum status: complessit√†=25/25, esempi=1081448/1081448 (100.0%)\n",
      "2025-06-24 13:05:06,224 [INFO] [Epoca 20] Curriculum status: complessit√†=22/22, esempi=120161/120161 (100.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training plots at epoch 20\n",
      "Training completed in 2.73 hours\n",
      "Best validation loss: 0.6731 at epoch 20\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Crea una cartella per i grafici se non esiste\n",
    "os.makedirs('training_plots', exist_ok=True)\n",
    "\n",
    "# Inizializza liste per memorizzare le metriche\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "epochs_list = []\n",
    "learning_rates = []\n",
    "complexity_values = []\n",
    "coverage_percentages = []\n",
    "\n",
    "model = build_improved_model(len(vocab))\n",
    "train_gen = CurriculumSmilesGenerator(train_data, vocab)\n",
    "val_gen = CurriculumSmilesGenerator(val_data, vocab)\n",
    "\n",
    "train_gen.update_complexity(0)\n",
    "val_gen.update_complexity(0)\n",
    "\n",
    "monitor_callback = EnhancedTrainingMonitor(val_gen)\n",
    "\n",
    "callbacks = [\n",
    "    monitor_callback,\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6),\n",
    "    CustomTensorBoard(log_dir='./logs'),\n",
    "    tf.keras.callbacks.ModelCheckpoint('diff_scaffold_2.h5', save_best_only=True, monitor='val_loss')\n",
    "]\n",
    "\n",
    "steps_per_epoch = len(train_data) // config.BATCH_SIZE\n",
    "val_steps = len(val_data) // config.BATCH_SIZE\n",
    "total_epochs = config.EPOCHS\n",
    "start_training = time.time()\n",
    "\n",
    "def plot_training_progress(save_path='training_plots/current_progress.png'):\n",
    "    \"\"\"Genera e salva un grafico con l'andamento del training\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Plot delle loss\n",
    "    ax1.plot(epochs_list, train_losses, 'b-', label='Training Loss')\n",
    "    ax1.plot(epochs_list, val_losses, 'r-', label='Validation Loss')\n",
    "    ax1.set_title('Model Loss During Training')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot secondario: Learning rate e complessit√† curriculum\n",
    "    ax2.set_title('Learning Rate and Curriculum Complexity')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    \n",
    "    # Learning rate (asse y sinistro)\n",
    "    color = 'tab:green'\n",
    "    ax2.set_ylabel('Learning Rate', color=color)\n",
    "    ax2.plot(epochs_list, learning_rates, color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.set_yscale('log')  # Scala logaritmica per LR\n",
    "    \n",
    "    # Complessit√† curriculum (asse y destro)\n",
    "    if complexity_values:\n",
    "        ax3 = ax2.twinx()\n",
    "        color = 'tab:orange'\n",
    "        ax3.set_ylabel('Complexity / Coverage %', color=color)\n",
    "        ax3.plot(epochs_list, complexity_values, 'o-', color=color, label='Complexity')\n",
    "        if coverage_percentages:\n",
    "            ax3.plot(epochs_list, coverage_percentages, 's-', color='tab:purple', label='Coverage %')\n",
    "        ax3.tick_params(axis='y', labelcolor=color)\n",
    "        ax3.legend(loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "for epoch in range(1, total_epochs + 1):\n",
    "    epoch_start = time.time()\n",
    "    current_history = model.fit(\n",
    "        train_gen.get_dataset(),\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        callbacks=callbacks,\n",
    "        epochs=1,\n",
    "        validation_data=val_gen.get_dataset(),\n",
    "        validation_steps=val_steps,\n",
    "        verbose=2,\n",
    "    )\n",
    "    epoch_duration = time.time() - epoch_start\n",
    "    \n",
    "    # Salva le metriche di questa epoca\n",
    "    current_train_loss = current_history.history['loss'][0]\n",
    "    current_val_loss = current_history.history['val_loss'][0]\n",
    "    \n",
    "    # Ottieni il learning rate corrente\n",
    "    if hasattr(model.optimizer, 'learning_rate'):\n",
    "        if isinstance(model.optimizer.learning_rate, tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "            current_lr = model.optimizer.learning_rate(epoch).numpy()\n",
    "        else:\n",
    "            current_lr = model.optimizer.learning_rate.numpy()\n",
    "    else:\n",
    "        current_lr = 0.001  # Default fallback\n",
    "    \n",
    "    # Aggiorna le liste per il plotting\n",
    "    train_losses.append(current_train_loss)\n",
    "    val_losses.append(current_val_loss)\n",
    "    learning_rates.append(current_lr)\n",
    "    epochs_list.append(epoch)\n",
    "    \n",
    "    # Aggiungi informazioni sul curriculum learning\n",
    "    if hasattr(train_gen, 'current_complexity') and hasattr(train_gen, 'max_complexity'):\n",
    "        complexity_values.append(train_gen.current_complexity)\n",
    "        if hasattr(train_gen, 'available_indices') and hasattr(train_gen, 'data_with_complexity'):\n",
    "            coverage = len(train_gen.available_indices) / len(train_gen.data_with_complexity) * 100\n",
    "            coverage_percentages.append(coverage)\n",
    "    \n",
    "    # Calcola la differenza di loss per il curriculum learning\n",
    "    if epoch > 1:\n",
    "        loss_diff = abs(current_val_loss - prev_val_loss) / prev_val_loss\n",
    "    else:\n",
    "        loss_diff = None\n",
    "    prev_val_loss = current_val_loss\n",
    "\n",
    "    # Aggiorna la complessit√†\n",
    "    train_gen.update_complexity(epoch, loss_diff=loss_diff)\n",
    "    val_gen.update_complexity(epoch, loss_diff=loss_diff)\n",
    "    \n",
    "    # Genera e salva il grafico ogni 5 epoche o all'ultima epoca\n",
    "    if epoch % 5 == 0 or epoch == total_epochs:\n",
    "        # Salva un grafico con il progresso corrente\n",
    "        plot_training_progress(f'training_plots/progress_epoch_{epoch}.png')\n",
    "        \n",
    "        # Salva anche un grafico sempre con lo stesso nome per facile accesso\n",
    "        plot_training_progress()\n",
    "        \n",
    "        print(f\"Saved training plots at epoch {epoch}\")\n",
    "\n",
    "# Fine training - salva il grafico finale\n",
    "plot_training_progress('training_plots/final_training_progress.png')\n",
    "\n",
    "total_duration = time.time() - start_training\n",
    "print(f\"Training completed in {total_duration/3600:.2f} hours\")\n",
    "print(f\"Best validation loss: {min(val_losses):.4f} at epoch {val_losses.index(min(val_losses))+1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe53fbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dopo il training, per generare SMILES e calcolare le metriche:\n",
    "generated, valid = monitor_callback.generate_smiles(10000)  # genera 1000 SMILES\n",
    "\n",
    "# Calcola validit√†\n",
    "validity = len(valid) / 10000\n",
    "\n",
    "# Calcola unicit√† (solo tra quelle valide)\n",
    "unique_valid = len(set(valid))\n",
    "uniqueness = unique_valid / len(valid) if valid else 0\n",
    "\n",
    "print(f\"Validit√†: {validity*100:.2f}%\")\n",
    "print(f\"Unicit√†: {uniqueness*100:.2f}%\")\n",
    "\n",
    "# Per la diversit√†, puoi per esempio calcolare le fingerprint e misurare la similarit√† media.\n",
    "# Questo √® un esempio con RDKit (assicurati di avere rdkit installato):\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, DataStructs\n",
    "\n",
    "fps = []\n",
    "for s in valid:\n",
    "    mol = Chem.MolFromSmiles(s)\n",
    "    if mol:\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2)\n",
    "        fps.append(fp)\n",
    "\n",
    "# Calcola la similarit√† media tra tutte le coppie\n",
    "similarities = []\n",
    "for i in range(len(fps)):\n",
    "    for j in range(i+1, len(fps)):\n",
    "        sim = DataStructs.TanimotoSimilarity(fps[i], fps[j])\n",
    "        similarities.append(sim)\n",
    "        \n",
    "avg_similarity = np.mean(similarities) if similarities else 0\n",
    "diversity = 1 - avg_similarity\n",
    "\n",
    "print(f\"Diversit√† (1 - similarit√† media): {diversity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f118703e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/grad/Desktop/pietro/denovo/ab/ab2'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c81ba74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
