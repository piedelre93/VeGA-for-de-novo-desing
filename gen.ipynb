{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "768b6b7e-22ce-4f74-8f8a-6aa132233a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/grad/Desktop/pietro/denovo/new/attachments_3/2/da_caricare/model_COCONUT/model_coconut.h5\"\n",
    "char2idx_path = \"/home/grad/Desktop/pietro/denovo/new/attachments_3/2/da_caricare/model_COCONUT/char2idx_coco.pkl\"\n",
    "idx2char_path = \"/home/grad/Desktop/pietro/denovo/new/attachments_3/2/da_caricare/model_COCONUT/idx2char_coco.pkl\"\n",
    "vocab_path    = \"/home/grad/Desktop/pietro/denovo/new/attachments_3/2/da_caricare/model_COCONUT/vocab_coco.json\"\n",
    "max_length = 20\n",
    "training_file = \"/home/grad/Desktop/pietro/denovo/s4-for-de-novo-drug-design/datasets/pkm2/train.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f7059cb-6019-406a-994b-8bc881480a99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model loaded from folder: /home/grad/Desktop/pietro/denovo/new/attachments_3/2/da_caricare/model_COCONUT/model_coconut.h5\n",
      "Loading and canonicalizing SMILES extracted from file: /home/grad/Desktop/pietro/denovo/s4-for-de-novo-drug-design/datasets/pkm2/train.txt\n",
      "Processed 436 rows, loaded 435 valid canonical SMILES from training set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import QED\n",
    "import csv\n",
    "\n",
    "# Se vuoi forzare l'uso della CPU, decommenta la seguente riga:\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "# --- Definizione delle custom objects (devono essere identiche a quelle usate in training) ---\n",
    "from tensorflow.keras.layers import Layer, Dense, Dropout, Embedding, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "class DynamicPositionalEncoding(Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super(DynamicPositionalEncoding, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "    def build(self, input_shape):\n",
    "        max_seq_len = input_shape[1]\n",
    "        pos = np.arange(max_seq_len)[:, np.newaxis]\n",
    "        i = np.arange(self.embed_dim)[np.newaxis, :]\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(self.embed_dim))\n",
    "        angle_rads = pos * angle_rates\n",
    "        angle_rads[:, 0::2] = tf.math.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = tf.math.cos(angle_rads[:, 1::2])\n",
    "        self.pos_encoding = tf.cast(angle_rads[np.newaxis, ...], dtype=tf.float32)\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "    def get_config(self):\n",
    "        config = super(DynamicPositionalEncoding, self).get_config()\n",
    "        config.update({'embed_dim': self.embed_dim})\n",
    "        return config\n",
    "\n",
    "class ImprovedTransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim, rate=0.1, **kwargs):\n",
    "        super(ImprovedTransformerBlock, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.rate = rate\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, dropout=rate)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ffn_dim, activation=\"gelu\"),\n",
    "            Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "    def call(self, inputs, training=False):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        # Crea la maschera causale (triangolare inferiore)\n",
    "        causal_mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        attn_output = self.mha(inputs, inputs, attention_mask=causal_mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    def get_config(self):\n",
    "        config = super(ImprovedTransformerBlock, self).get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"ffn_dim\": self.ffn_dim,\n",
    "            \"rate\": self.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, embed_dim, warmup_steps=10000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.embed_dim = tf.cast(embed_dim, tf.float32)\n",
    "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32) + 1e-9\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.embed_dim) * tf.math.minimum(arg1, arg2)\n",
    "    def get_config(self):\n",
    "        return {\"embed_dim\": self.embed_dim.numpy(), \"warmup_steps\": self.warmup_steps.numpy()}\n",
    "\n",
    "def smoothed_loss(y_true, y_pred):\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "    return tf.reduce_sum(loss * mask) / (tf.reduce_sum(mask) + 1e-9)\n",
    "\n",
    "custom_objects = {\n",
    "    \"DynamicPositionalEncoding\": DynamicPositionalEncoding,\n",
    "    \"ImprovedTransformerBlock\": ImprovedTransformerBlock,\n",
    "    \"CustomSchedule\": CustomSchedule,\n",
    "    \"smoothed_loss\": smoothed_loss,\n",
    "}\n",
    "\n",
    "# --- Caricamento del modello fine-tuned ---\n",
    "model = load_model(model_path, custom_objects=custom_objects)\n",
    "print(\"Trained model loaded from folder:\", model_path)\n",
    "\n",
    "# --- Caricamento delle mappature e del vocabolario ---\n",
    "with open(char2idx_path, \"rb\") as f:\n",
    "    char2idx = pickle.load(f)\n",
    "with open(idx2char_path, \"rb\") as f:\n",
    "    idx2char = pickle.load(f)\n",
    "with open(vocab_path, \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "# Imposta la lunghezza massima (deve essere la stessa usata in training)\n",
    "\n",
    "# --- Funzione per generare un batch di SMILES parallelamente ---\n",
    "def generate_smiles_batch(model, char2idx, idx2char, max_length, batch_size=64, temperature=1.0):\n",
    "    input_seqs = np.full((batch_size, max_length), char2idx['<PAD>'], dtype=np.int32)\n",
    "    input_seqs[:, 0] = char2idx['<START>']\n",
    "    finished = np.zeros(batch_size, dtype=bool)\n",
    "    end_token = char2idx['<END>']\n",
    "\n",
    "    for t in range(1, max_length):\n",
    "        # Otteniamo tutti i logits in una singola chiamata\n",
    "        logits = model.predict(input_seqs, verbose=0)  # shape (batch_size, max_length, vocab_size)\n",
    "        step_logits = logits[:, t-1, :]\n",
    "        step_probs = tf.nn.softmax(step_logits / temperature).numpy()\n",
    "\n",
    "        # Aggiorniamo tutti i batch\n",
    "        for i in range(batch_size):\n",
    "            if not finished[i]:\n",
    "                sampled = np.random.choice(len(step_probs[i]), p=step_probs[i])\n",
    "                input_seqs[i, t] = sampled\n",
    "                if sampled == end_token:\n",
    "                    finished[i] = True\n",
    "        if finished.all():\n",
    "            break\n",
    "\n",
    "    smiles_list = []\n",
    "    for seq in input_seqs:\n",
    "        tokens = [idx2char[idx] for idx in seq\n",
    "                  if idx not in {char2idx['<PAD>'], char2idx['<START>'], char2idx['<END>']}]\n",
    "        smi = ''.join(tokens)\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol:\n",
    "            smi = Chem.MolToSmiles(mol, canonical=True)\n",
    "        smiles_list.append(smi)\n",
    "\n",
    "    return smiles_list\n",
    "\n",
    "# --- Funzione per valutare e salvare i batch di SMILES ---\n",
    "# --- Funzione per valutare e salvare i batch di SMILES ---\n",
    "def evaluate_and_save_batches(model, char2idx, idx2char, max_length,\n",
    "                              training_smiles_set, out_csv_path,\n",
    "                              num_batches=10, batch_size=64, temperature=1.0):\n",
    "    all_generated = []\n",
    "    for b in range(num_batches):\n",
    "        generated = generate_smiles_batch(model, char2idx, idx2char,\n",
    "                                          max_length, batch_size, temperature)\n",
    "        all_generated.extend(generated)\n",
    "        print(f\"Batch {b+1}/{num_batches}: generated {len(generated)} SMILES\")\n",
    "\n",
    "    # Filtra e canonicalizza le SMILES valide\n",
    "    valid_smiles = []\n",
    "    for smi in all_generated:\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol:\n",
    "            canon = Chem.MolToSmiles(mol, canonical=True)\n",
    "            valid_smiles.append(canon)\n",
    "\n",
    "    # Calcola novelty RAW (prima di rimuovere i duplicati)\n",
    "    if valid_smiles:\n",
    "        novelty_raw = sum(1 for smi in valid_smiles if smi not in training_smiles_set) / len(valid_smiles)\n",
    "    else:\n",
    "        novelty_raw = 0.0\n",
    "\n",
    "    # Rimuovi duplicati per ulteriori metriche\n",
    "    unique_smiles = list(set(valid_smiles))\n",
    "\n",
    "    # Valuta QED e SA sulle valide uniche\n",
    "    qed_list, sa_list = [], []\n",
    "    for smi in unique_smiles:\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol:\n",
    "            try:\n",
    "                qed_list.append(QED.qed(mol))\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                sa_list.append(sascorer.calculateScore(mol))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    # Salva su CSV\n",
    "    with open(out_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for smi in unique_smiles:\n",
    "            writer.writerow([smi])\n",
    "    print(f\"Saved {len(unique_smiles)} unique SMILES to {out_csv_path}\")\n",
    "\n",
    "    # Altre metriche\n",
    "    validity = len(valid_smiles) / len(all_generated) if all_generated else 0\n",
    "    avg_qed = np.mean(qed_list) if qed_list else 0\n",
    "    avg_sa = np.mean(sa_list) if sa_list else 0\n",
    "    novelty_unique = sum(1 for smi in unique_smiles if smi not in training_smiles_set) / len(unique_smiles) if unique_smiles else 0\n",
    "    originality = len(unique_smiles) / len(valid_smiles) if valid_smiles else 0\n",
    "\n",
    "    print(f\"\"\"\n",
    "Molecule Generation Report:\n",
    "  Total generated:         {len(all_generated)}\n",
    "  Validity:               {validity*100:.2f}% ({len(valid_smiles)}/{len(all_generated)})\n",
    "  Unique valid:           {len(unique_smiles)}\n",
    "  Average QED:            {avg_qed:.4f}\n",
    "  Average SA:             {avg_sa:.4f}\n",
    "  Novelty (unique):       {novelty_unique*100:.2f}% \n",
    "  Originality:            {originality*100:.2f}% \n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Caricamento del set di SMILES del training (per calcolare novelty) ---\n",
    "# --- Caricamento del set di SMILES del training (per calcolare novelty) ---\n",
    "training_smiles_set = set()\n",
    "print(f\"Loading and canonicalizing SMILES extracted from file: {training_file}\")\n",
    "count_processed = 0\n",
    "count_valid = 0\n",
    "if os.path.exists(training_file):\n",
    "    with open(training_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            count_processed += 1\n",
    "            smi = line.strip()\n",
    "            if not smi:\n",
    "                continue\n",
    "            try:\n",
    "                mol = Chem.MolFromSmiles(smi)\n",
    "                if mol:\n",
    "                    # Canonicalizza prima di aggiungere al set\n",
    "                    canon_smi = Chem.MolToSmiles(mol, canonical=True)\n",
    "                    training_smiles_set.add(canon_smi)\n",
    "                    count_valid += 1\n",
    "                # else: # Opzionale: loggare SMILES non valide nel training set\n",
    "                #     print(f\"Attenzione: SMILES non valida nel training set ignorata: {smi}\")\n",
    "            except Exception as e:\n",
    "                # Opzionale: loggare errori di parsing\n",
    "                # print(f\"Errore nel processare la SMILES '{smi}': {e}\")\n",
    "                pass # Ignora SMILES che causano errori\n",
    "    print(f\"Processed {count_processed} rows, loaded {len(training_smiles_set)} valid canonical SMILES from training set.\")\n",
    "else:\n",
    "    print(f\"Warning: Training file '{training_file}' not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd0e1bd3-1168-48ae-8109-311b8d70c474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Esempio di esecuzione ---\n",
    "num_batches   = 10\n",
    "batch_size    = 100\n",
    "temperature   = 0.6\n",
    "out_csv_path  = \"/home/grad/Desktop/pietro/denovo/new/risultati/fine/gba/generated_f.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14d1f76a-19b5-4e4b-912f-eea40a59e36b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"functional_4\" is incompatible with the layer: expected shape=(None, 193), found shape=(32, 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m RDLogger\u001b[38;5;241m.\u001b[39mDisableLog(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrdApp.*\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msascorer\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mevaluate_and_save_batches\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar2idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx2char\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_smiles_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_csv_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 155\u001b[0m, in \u001b[0;36mevaluate_and_save_batches\u001b[0;34m(model, char2idx, idx2char, max_length, training_smiles_set, out_csv_path, num_batches, batch_size, temperature)\u001b[0m\n\u001b[1;32m    153\u001b[0m all_generated \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[0;32m--> 155\u001b[0m     generated \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_smiles_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar2idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx2char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     all_generated\u001b[38;5;241m.\u001b[39mextend(generated)\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mb\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_batches\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: generated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(generated)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m SMILES\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 122\u001b[0m, in \u001b[0;36mgenerate_smiles_batch\u001b[0;34m(model, char2idx, idx2char, max_length, batch_size, temperature)\u001b[0m\n\u001b[1;32m    118\u001b[0m end_token \u001b[38;5;241m=\u001b[39m char2idx[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<END>\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_length):\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# Otteniamo tutti i logits in una singola chiamata\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape (batch_size, max_length, vocab_size)\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     step_logits \u001b[38;5;241m=\u001b[39m logits[:, t\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m    124\u001b[0m     step_probs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(step_logits \u001b[38;5;241m/\u001b[39m temperature)\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/keras/src/layers/input_spec.py:245\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[0;32m--> 245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    246\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"functional_4\" is incompatible with the layer: expected shape=(None, 193), found shape=(32, 20)"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem import QED, RDConfig\n",
    "sys.path.append(os.path.join(RDConfig.RDContribDir, 'SA_Score'))\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "import sascorer\n",
    "evaluate_and_save_batches(\n",
    "    model, char2idx, idx2char, max_length,\n",
    "    training_smiles_set, out_csv_path,\n",
    "    num_batches=num_batches,\n",
    "    batch_size=batch_size,\n",
    "    temperature=temperature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6713b7d4-63bc-49bb-b854-1b473084bcba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19766ef-53aa-41d8-943c-31b24e0793b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20db8cb2-1819-407a-ad34-59cd0250d5d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c89d17-7535-445b-907e-304fe56b4bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cec5f8-8fff-4e5f-8b43-2736fa5a1932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
