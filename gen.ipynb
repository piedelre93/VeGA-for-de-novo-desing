{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "768b6b7e-22ce-4f74-8f8a-6aa132233a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/grad/Desktop/pietro/denovo/new/risultati/fine/gba/model_final_fxar.h5\"\n",
    "char2idx_path = \"/home/grad/Desktop/pietro/denovo/new/attachments_3/2/da_caricare/originale/char2idx.pkl\"\n",
    "idx2char_path = \"/home/grad/Desktop/pietro/denovo/new/attachments_3/2/da_caricare/originale/idx2char.pkl\"\n",
    "vocab_path    = \"/home/grad/Desktop/pietro/denovo/new/attachments_3/2/da_caricare/originale/vocab.json\"\n",
    "max_length = 90\n",
    "training_file = \"/home/grad/Desktop/pietro/denovo/s4-for-de-novo-drug-design/datasets/fxar/fine.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f7059cb-6019-406a-994b-8bc881480a99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 15:23:23.571198: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-03 15:23:23.584758: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751549003.601570 2810360 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751549003.606553 2810360 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1751549003.622492 2810360 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751549003.622519 2810360 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751549003.622521 2810360 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751549003.622523 2810360 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-03 15:23:23.630371: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "I0000 00:00:1751549006.150048 2810360 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3811 MB memory:  -> device: 0, name: NVIDIA RTX A2000 12GB, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
      "/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/keras/src/layers/layer.py:939: UserWarning: Layer 'dynamic_positional_encoding' (of type DynamicPositionalEncoding) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "[15:23:27] SMILES Parse Error: syntax error while parsing: r\n",
      "[15:23:27] SMILES Parse Error: Failed parsing SMILES 'r' for input: 'r'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model loaded from folder: /home/grad/Desktop/pietro/denovo/new/risultati/fine/gba/model_final_fxar.h5\n",
      "Loading and canonicalizing SMILES extracted from file: /home/grad/Desktop/pietro/denovo/s4-for-de-novo-drug-design/datasets/fxar/fine.csv\n",
      "Processed 883 rows, loaded 882 valid canonical SMILES from training set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import QED\n",
    "import csv\n",
    "\n",
    "# Se vuoi forzare l'uso della CPU, decommenta la seguente riga:\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "# --- Definizione delle custom objects (devono essere identiche a quelle usate in training) ---\n",
    "from tensorflow.keras.layers import Layer, Dense, Dropout, Embedding, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "class DynamicPositionalEncoding(Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super(DynamicPositionalEncoding, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "    def build(self, input_shape):\n",
    "        max_seq_len = input_shape[1]\n",
    "        pos = np.arange(max_seq_len)[:, np.newaxis]\n",
    "        i = np.arange(self.embed_dim)[np.newaxis, :]\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(self.embed_dim))\n",
    "        angle_rads = pos * angle_rates\n",
    "        angle_rads[:, 0::2] = tf.math.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = tf.math.cos(angle_rads[:, 1::2])\n",
    "        self.pos_encoding = tf.cast(angle_rads[np.newaxis, ...], dtype=tf.float32)\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "    def get_config(self):\n",
    "        config = super(DynamicPositionalEncoding, self).get_config()\n",
    "        config.update({'embed_dim': self.embed_dim})\n",
    "        return config\n",
    "\n",
    "class ImprovedTransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim, rate=0.1, **kwargs):\n",
    "        super(ImprovedTransformerBlock, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.rate = rate\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, dropout=rate)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ffn_dim, activation=\"gelu\"),\n",
    "            Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "    def call(self, inputs, training=False):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        # Crea la maschera causale (triangolare inferiore)\n",
    "        causal_mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        attn_output = self.mha(inputs, inputs, attention_mask=causal_mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    def get_config(self):\n",
    "        config = super(ImprovedTransformerBlock, self).get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"ffn_dim\": self.ffn_dim,\n",
    "            \"rate\": self.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, embed_dim, warmup_steps=10000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.embed_dim = tf.cast(embed_dim, tf.float32)\n",
    "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32) + 1e-9\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.embed_dim) * tf.math.minimum(arg1, arg2)\n",
    "    def get_config(self):\n",
    "        return {\"embed_dim\": self.embed_dim.numpy(), \"warmup_steps\": self.warmup_steps.numpy()}\n",
    "\n",
    "def smoothed_loss(y_true, y_pred):\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "    return tf.reduce_sum(loss * mask) / (tf.reduce_sum(mask) + 1e-9)\n",
    "\n",
    "custom_objects = {\n",
    "    \"DynamicPositionalEncoding\": DynamicPositionalEncoding,\n",
    "    \"ImprovedTransformerBlock\": ImprovedTransformerBlock,\n",
    "    \"CustomSchedule\": CustomSchedule,\n",
    "    \"smoothed_loss\": smoothed_loss,\n",
    "}\n",
    "\n",
    "# --- Caricamento del modello fine-tuned ---\n",
    "model = load_model(model_path, custom_objects=custom_objects)\n",
    "print(\"Trained model loaded from folder:\", model_path)\n",
    "\n",
    "# --- Caricamento delle mappature e del vocabolario ---\n",
    "with open(char2idx_path, \"rb\") as f:\n",
    "    char2idx = pickle.load(f)\n",
    "with open(idx2char_path, \"rb\") as f:\n",
    "    idx2char = pickle.load(f)\n",
    "with open(vocab_path, \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "# Imposta la lunghezza massima (deve essere la stessa usata in training)\n",
    "\n",
    "# --- Funzione per generare un batch di SMILES parallelamente ---\n",
    "def generate_smiles_batch(model, char2idx, idx2char, max_length, batch_size=64, temperature=1.0):\n",
    "    input_seqs = np.full((batch_size, max_length), char2idx['<PAD>'], dtype=np.int32)\n",
    "    input_seqs[:, 0] = char2idx['<START>']\n",
    "    finished = np.zeros(batch_size, dtype=bool)\n",
    "    end_token = char2idx['<END>']\n",
    "\n",
    "    for t in range(1, max_length):\n",
    "        # Otteniamo tutti i logits in una singola chiamata\n",
    "        logits = model.predict(input_seqs, verbose=0)  # shape (batch_size, max_length, vocab_size)\n",
    "        step_logits = logits[:, t-1, :]\n",
    "        step_probs = tf.nn.softmax(step_logits / temperature).numpy()\n",
    "\n",
    "        # Aggiorniamo tutti i batch\n",
    "        for i in range(batch_size):\n",
    "            if not finished[i]:\n",
    "                sampled = np.random.choice(len(step_probs[i]), p=step_probs[i])\n",
    "                input_seqs[i, t] = sampled\n",
    "                if sampled == end_token:\n",
    "                    finished[i] = True\n",
    "        if finished.all():\n",
    "            break\n",
    "\n",
    "    smiles_list = []\n",
    "    for seq in input_seqs:\n",
    "        tokens = [idx2char[idx] for idx in seq\n",
    "                  if idx not in {char2idx['<PAD>'], char2idx['<START>'], char2idx['<END>']}]\n",
    "        smi = ''.join(tokens)\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol:\n",
    "            smi = Chem.MolToSmiles(mol, canonical=True)\n",
    "        smiles_list.append(smi)\n",
    "\n",
    "    return smiles_list\n",
    "\n",
    "# --- Funzione per valutare e salvare i batch di SMILES ---\n",
    "# --- Funzione per valutare e salvare i batch di SMILES ---\n",
    "def evaluate_and_save_batches(model, char2idx, idx2char, max_length,\n",
    "                              training_smiles_set, out_csv_path,\n",
    "                              num_batches=10, batch_size=64, temperature=1.0):\n",
    "    all_generated = []\n",
    "    for b in range(num_batches):\n",
    "        generated = generate_smiles_batch(model, char2idx, idx2char,\n",
    "                                          max_length, batch_size, temperature)\n",
    "        all_generated.extend(generated)\n",
    "        print(f\"Batch {b+1}/{num_batches}: generated {len(generated)} SMILES\")\n",
    "\n",
    "    # Filtra e canonicalizza le SMILES valide\n",
    "    valid_smiles = []\n",
    "    for smi in all_generated:\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol:\n",
    "            canon = Chem.MolToSmiles(mol, canonical=True)\n",
    "            valid_smiles.append(canon)\n",
    "\n",
    "    # Calcola novelty PRIMA di rimuovere duplicati\n",
    "    if valid_smiles:\n",
    "        num_novel = sum(1 for smi in valid_smiles if smi not in training_smiles_set)\n",
    "        novelty_raw = num_novel / len(valid_smiles)\n",
    "    else:\n",
    "        novelty_raw = 0.0\n",
    "\n",
    "    # Rimuovi duplicati e molecole già presenti nel training set\n",
    "    unique_and_novel = list({smi for smi in valid_smiles if smi not in training_smiles_set})\n",
    "\n",
    "    # Valuta QED e SA\n",
    "    qed_list, sa_list = [], []\n",
    "    for smi in unique_and_novel:\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol:\n",
    "            try:\n",
    "                qed_list.append(QED.qed(mol))\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                sa_list.append(sascorer.calculateScore(mol))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    # Salva su CSV solo SMILES uniche e nuove\n",
    "    with open(out_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for smi in unique_and_novel:\n",
    "            writer.writerow([smi])\n",
    "    print(f\"Saved {len(unique_and_novel)} unique and novel SMILES to {out_csv_path}\")\n",
    "\n",
    "    # Metriche\n",
    "    validity = len(valid_smiles) / len(all_generated) if all_generated else 0\n",
    "    avg_qed = np.mean(qed_list) if qed_list else 0\n",
    "    avg_sa = np.mean(sa_list) if sa_list else 0\n",
    "    originality = len(unique_and_novel) / len(valid_smiles) if valid_smiles else 0\n",
    "\n",
    "    print(f\"\"\"\n",
    "Molecule Generation Report:\n",
    "  Total generated:         {len(all_generated)}\n",
    "  Validity:               {validity*100:.2f}% ({len(valid_smiles)}/{len(all_generated)})\n",
    "  Unique & novel:         {len(unique_and_novel)}\n",
    "  Average QED:            {avg_qed:.4f}\n",
    "  Average SA:             {avg_sa:.4f}\n",
    "  Novelty (raw):          {novelty_raw*100:.2f}% \n",
    "  Originality (final):    {originality*100:.2f}% \n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Caricamento del set di SMILES del training (per calcolare novelty) ---\n",
    "# --- Caricamento del set di SMILES del training (per calcolare novelty) ---\n",
    "training_smiles_set = set()\n",
    "\n",
    "print(f\"Loading and canonicalizing SMILES extracted from file: {training_file}\")\n",
    "count_processed = 0\n",
    "count_valid = 0\n",
    "if os.path.exists(training_file):\n",
    "    with open(training_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            count_processed += 1\n",
    "            smi = line.strip()\n",
    "            if not smi:\n",
    "                continue\n",
    "            try:\n",
    "                mol = Chem.MolFromSmiles(smi)\n",
    "                if mol:\n",
    "                    # Canonicalizza prima di aggiungere al set\n",
    "                    canon_smi = Chem.MolToSmiles(mol, canonical=True)\n",
    "                    training_smiles_set.add(canon_smi)\n",
    "                    count_valid += 1\n",
    "                # else: # Opzionale: loggare SMILES non valide nel training set\n",
    "                #     print(f\"Attenzione: SMILES non valida nel training set ignorata: {smi}\")\n",
    "            except Exception as e:\n",
    "                # Opzionale: loggare errori di parsing\n",
    "                # print(f\"Errore nel processare la SMILES '{smi}': {e}\")\n",
    "                pass # Ignora SMILES che causano errori\n",
    "    print(f\"Processed {count_processed} rows, loaded {len(training_smiles_set)} valid canonical SMILES from training set.\")\n",
    "else:\n",
    "    print(f\"Warning: Training file '{training_file}' not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd0e1bd3-1168-48ae-8109-311b8d70c474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Esempio di esecuzione ---\n",
    "num_batches   = 10\n",
    "batch_size    = 1000\n",
    "temperature   = 1\n",
    "out_csv_path  = \"/home/grad/Desktop/pietro/denovo/s4-for-de-novo-drug-design/s4_loro/gen_mio/10000/gen_fxr_miol1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14d1f76a-19b5-4e4b-912f-eea40a59e36b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1751549008.374322 2879859 service.cc:152] XLA service 0x779fd00035d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1751549008.374337 2879859 service.cc:160]   StreamExecutor device (0): NVIDIA RTX A2000 12GB, Compute Capability 8.6\n",
      "2025-07-03 15:23:28.416286: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1751549008.509414 2879859 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-07-03 15:23:29.936781: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_37', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2025-07-03 15:23:30.379232: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_37', 148 bytes spill stores, 148 bytes spill loads\n",
      "\n",
      "2025-07-03 15:23:30.442612: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_37_0', 700 bytes spill stores, 700 bytes spill loads\n",
      "\n",
      "2025-07-03 15:23:31.024890: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_37', 472 bytes spill stores, 472 bytes spill loads\n",
      "\n",
      "2025-07-03 15:23:31.177381: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_37', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-07-03 15:23:31.346620: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_37', 792 bytes spill stores, 792 bytes spill loads\n",
      "\n",
      "2025-07-03 15:23:31.360781: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_37', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-07-03 15:23:31.442996: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_37', 632 bytes spill stores, 632 bytes spill loads\n",
      "\n",
      "I0000 00:00:1751549013.014564 2879859 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2025-07-03 15:23:35.568504: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_37', 792 bytes spill stores, 792 bytes spill loads\n",
      "\n",
      "2025-07-03 15:23:35.732329: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_37', 632 bytes spill stores, 632 bytes spill loads\n",
      "\n",
      "2025-07-03 15:23:35.985834: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_37', 148 bytes spill stores, 148 bytes spill loads\n",
      "\n",
      "2025-07-03 15:23:36.046132: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_37', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-07-03 15:23:36.315873: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_37', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2025-07-03 15:23:36.507000: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_37', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-07-03 15:23:36.527808: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_37', 472 bytes spill stores, 472 bytes spill loads\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem import QED, RDConfig\n",
    "sys.path.append(os.path.join(RDConfig.RDContribDir, 'SA_Score'))\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "import sascorer\n",
    "evaluate_and_save_batches(\n",
    "    model, char2idx, idx2char, max_length,\n",
    "    training_smiles_set, out_csv_path,\n",
    "    num_batches=num_batches,\n",
    "    batch_size=batch_size,\n",
    "    temperature=temperature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6713b7d4-63bc-49bb-b854-1b473084bcba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a19766ef-53aa-41d8-943c-31b24e0793b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/grad/Desktop/pietro/denovo/new/attachments_3/2/da_caricare'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20db8cb2-1819-407a-ad34-59cd0250d5d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c89d17-7535-445b-907e-304fe56b4bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cec5f8-8fff-4e5f-8b43-2736fa5a1932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
