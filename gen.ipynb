{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "768b6b7e-22ce-4f74-8f8a-6aa132233a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/grad/Desktop/pietro/denovo/new/risultati/fine/gba/model_final_fxar_pp_2.h5\"\n",
    "char2idx_path = \"/home/grad/Desktop/pietro/denovo/ab/ab3/char2idx.pkl\"\n",
    "idx2char_path = \"/home/grad/Desktop/pietro/denovo/ab/ab3/idx2char.pkl\"\n",
    "vocab_path    = \"/home/grad/Desktop/pietro/denovo/ab/ab3/vocab.json\"\n",
    "max_length = 90\n",
    "training_file = \"/home/grad/Desktop/pietro/denovo/s4-for-de-novo-drug-design/datasets/fxar/fine.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f7059cb-6019-406a-994b-8bc881480a99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 14:31:58.524750: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-24 14:31:58.538000: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750768318.554094 3395082 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750768318.558915 3395082 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750768318.571118 3395082 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750768318.571137 3395082 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750768318.571138 3395082 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750768318.571139 3395082 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-24 14:31:58.575601: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "I0000 00:00:1750768321.161785 3395082 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6228 MB memory:  -> device: 0, name: NVIDIA RTX A2000 12GB, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
      "/home/grad/anaconda3/envs/smiles-transformer/lib/python3.10/site-packages/keras/src/layers/layer.py:939: UserWarning: Layer 'dynamic_positional_encoding_2' (of type DynamicPositionalEncoding) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model loaded from folder: /home/grad/Desktop/pietro/denovo/new/risultati/fine/gba/model_final_fxar_pp_2.h5\n",
      "Loading and canonicalizing SMILES extracted from file: /home/grad/Desktop/pietro/denovo/s4-for-de-novo-drug-design/datasets/fxar/fine.txt\n",
      "Processed 882 rows, loaded 882 valid canonical SMILES from training set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import QED\n",
    "import csv\n",
    "\n",
    "# Se vuoi forzare l'uso della CPU, decommenta la seguente riga:\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "# --- Definizione delle custom objects (devono essere identiche a quelle usate in training) ---\n",
    "from tensorflow.keras.layers import Layer, Dense, Dropout, Embedding, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "class DynamicPositionalEncoding(Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super(DynamicPositionalEncoding, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "    def build(self, input_shape):\n",
    "        max_seq_len = input_shape[1]\n",
    "        pos = np.arange(max_seq_len)[:, np.newaxis]\n",
    "        i = np.arange(self.embed_dim)[np.newaxis, :]\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(self.embed_dim))\n",
    "        angle_rads = pos * angle_rates\n",
    "        angle_rads[:, 0::2] = tf.math.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = tf.math.cos(angle_rads[:, 1::2])\n",
    "        self.pos_encoding = tf.cast(angle_rads[np.newaxis, ...], dtype=tf.float32)\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "    def get_config(self):\n",
    "        config = super(DynamicPositionalEncoding, self).get_config()\n",
    "        config.update({'embed_dim': self.embed_dim})\n",
    "        return config\n",
    "\n",
    "class ImprovedTransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim, rate=0.1, **kwargs):\n",
    "        super(ImprovedTransformerBlock, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.rate = rate\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, dropout=rate)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ffn_dim, activation=\"gelu\"),\n",
    "            Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "    def call(self, inputs, training=False):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        # Crea la maschera causale (triangolare inferiore)\n",
    "        causal_mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        attn_output = self.mha(inputs, inputs, attention_mask=causal_mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    def get_config(self):\n",
    "        config = super(ImprovedTransformerBlock, self).get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"ffn_dim\": self.ffn_dim,\n",
    "            \"rate\": self.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, embed_dim, warmup_steps=10000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.embed_dim = tf.cast(embed_dim, tf.float32)\n",
    "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32) + 1e-9\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.embed_dim) * tf.math.minimum(arg1, arg2)\n",
    "    def get_config(self):\n",
    "        return {\"embed_dim\": self.embed_dim.numpy(), \"warmup_steps\": self.warmup_steps.numpy()}\n",
    "\n",
    "def smoothed_loss(y_true, y_pred):\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "    return tf.reduce_sum(loss * mask) / (tf.reduce_sum(mask) + 1e-9)\n",
    "\n",
    "custom_objects = {\n",
    "    \"DynamicPositionalEncoding\": DynamicPositionalEncoding,\n",
    "    \"ImprovedTransformerBlock\": ImprovedTransformerBlock,\n",
    "    \"CustomSchedule\": CustomSchedule,\n",
    "    \"smoothed_loss\": smoothed_loss,\n",
    "}\n",
    "\n",
    "# --- Caricamento del modello fine-tuned ---\n",
    "model = load_model(model_path, custom_objects=custom_objects)\n",
    "print(\"Trained model loaded from folder:\", model_path)\n",
    "\n",
    "# --- Caricamento delle mappature e del vocabolario ---\n",
    "with open(char2idx_path, \"rb\") as f:\n",
    "    char2idx = pickle.load(f)\n",
    "with open(idx2char_path, \"rb\") as f:\n",
    "    idx2char = pickle.load(f)\n",
    "with open(vocab_path, \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "# Imposta la lunghezza massima (deve essere la stessa usata in training)\n",
    "\n",
    "# --- Funzione per generare un batch di SMILES parallelamente ---\n",
    "def generate_smiles_batch(model, char2idx, idx2char, max_length, batch_size=64, temperature=1.0):\n",
    "    input_seqs = np.full((batch_size, max_length), char2idx['<PAD>'], dtype=np.int32)\n",
    "    input_seqs[:, 0] = char2idx['<START>']\n",
    "    finished = np.zeros(batch_size, dtype=bool)\n",
    "    end_token = char2idx['<END>']\n",
    "\n",
    "    for t in range(1, max_length):\n",
    "        # Otteniamo tutti i logits in una singola chiamata\n",
    "        logits = model.predict(input_seqs, verbose=0)  # shape (batch_size, max_length, vocab_size)\n",
    "        step_logits = logits[:, t-1, :]\n",
    "        step_probs = tf.nn.softmax(step_logits / temperature).numpy()\n",
    "\n",
    "        # Aggiorniamo tutti i batch\n",
    "        for i in range(batch_size):\n",
    "            if not finished[i]:\n",
    "                sampled = np.random.choice(len(step_probs[i]), p=step_probs[i])\n",
    "                input_seqs[i, t] = sampled\n",
    "                if sampled == end_token:\n",
    "                    finished[i] = True\n",
    "        if finished.all():\n",
    "            break\n",
    "\n",
    "    smiles_list = []\n",
    "    for seq in input_seqs:\n",
    "        tokens = [idx2char[idx] for idx in seq\n",
    "                  if idx not in {char2idx['<PAD>'], char2idx['<START>'], char2idx['<END>']}]\n",
    "        smi = ''.join(tokens)\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol:\n",
    "            smi = Chem.MolToSmiles(mol, canonical=True)\n",
    "        smiles_list.append(smi)\n",
    "\n",
    "    return smiles_list\n",
    "\n",
    "# --- Funzione per valutare e salvare i batch di SMILES ---\n",
    "# --- Funzione per valutare e salvare i batch di SMILES ---\n",
    "def evaluate_and_save_batches(model, char2idx, idx2char, max_length,\n",
    "                              training_smiles_set, out_csv_path,\n",
    "                              num_batches=10, batch_size=64, temperature=1.0):\n",
    "    all_generated = []\n",
    "    for b in range(num_batches):\n",
    "        generated = generate_smiles_batch(model, char2idx, idx2char,\n",
    "                                          max_length, batch_size, temperature)\n",
    "        all_generated.extend(generated)\n",
    "        print(f\"Batch {b+1}/{num_batches}: generated {len(generated)} SMILES\")\n",
    "\n",
    "    # Filtra e canonicalizza le SMILES valide\n",
    "    valid_smiles = []\n",
    "    for smi in all_generated:\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol:\n",
    "            canon = Chem.MolToSmiles(mol, canonical=True)\n",
    "            valid_smiles.append(canon)\n",
    "\n",
    "    # Calcola novelty RAW (prima di rimuovere i duplicati)\n",
    "    if valid_smiles:\n",
    "        novelty_raw = sum(1 for smi in valid_smiles if smi not in training_smiles_set) / len(valid_smiles)\n",
    "    else:\n",
    "        novelty_raw = 0.0\n",
    "\n",
    "    # Rimuovi duplicati per ulteriori metriche\n",
    "    unique_smiles = list(set(valid_smiles))\n",
    "    \n",
    "    # AGGIUNTO QUI: Filtra solo le molecole novel (non presenti nel training set)\n",
    "    novel_smiles = [smi for smi in unique_smiles if smi not in training_smiles_set]\n",
    "\n",
    "    # Valuta QED e SA sulle valide uniche\n",
    "    qed_list, sa_list = [], []\n",
    "    for smi in unique_smiles:  # mantieni le metriche su tutte le uniche\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol:\n",
    "            try:\n",
    "                qed_list.append(QED.qed(mol))\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                sa_list.append(sascorer.calculateScore(mol))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    # Salva su CSV SOLO LE NOVEL\n",
    "    with open(out_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for smi in novel_smiles:  # Usa novel_smiles invece di unique_smiles\n",
    "            writer.writerow([smi])\n",
    "    print(f\"Saved {len(novel_smiles)} novel unique SMILES to {out_csv_path}\")\n",
    "\n",
    "    # Altre metriche\n",
    "    validity = len(valid_smiles) / len(all_generated) if all_generated else 0\n",
    "    avg_qed = np.mean(qed_list) if qed_list else 0\n",
    "    avg_sa = np.mean(sa_list) if sa_list else 0\n",
    "    novelty_unique = len(novel_smiles) / len(unique_smiles) if unique_smiles else 0\n",
    "    originality = len(unique_smiles) / len(valid_smiles) if valid_smiles else 0\n",
    "\n",
    "    print(f\"\"\"\n",
    "Molecule Generation Report:\n",
    "  Total generated:         {len(all_generated)}\n",
    "  Validity:               {validity*100:.2f}% ({len(valid_smiles)}/{len(all_generated)})\n",
    "  Unique valid:           {len(unique_smiles)}\n",
    "  Novel unique:           {len(novel_smiles)} ({novelty_unique*100:.2f}%)\n",
    "  Average QED:            {avg_qed:.4f}\n",
    "  Average SA:             {avg_sa:.4f}\n",
    "  Novelty (raw):          {novelty_raw*100:.2f}% \n",
    "  Originality:            {originality*100:.2f}% \n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Caricamento del set di SMILES del training (per calcolare novelty) ---\n",
    "# --- Caricamento del set di SMILES del training (per calcolare novelty) ---\n",
    "training_smiles_set = set()\n",
    "print(f\"Loading and canonicalizing SMILES extracted from file: {training_file}\")\n",
    "count_processed = 0\n",
    "count_valid = 0\n",
    "if os.path.exists(training_file):\n",
    "    with open(training_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            count_processed += 1\n",
    "            smi = line.strip()\n",
    "            if not smi:\n",
    "                continue\n",
    "            try:\n",
    "                mol = Chem.MolFromSmiles(smi)\n",
    "                if mol:\n",
    "                    # Canonicalizza prima di aggiungere al set\n",
    "                    canon_smi = Chem.MolToSmiles(mol, canonical=True)\n",
    "                    training_smiles_set.add(canon_smi)\n",
    "                    count_valid += 1\n",
    "                # else: # Opzionale: loggare SMILES non valide nel training set\n",
    "                #     print(f\"Attenzione: SMILES non valida nel training set ignorata: {smi}\")\n",
    "            except Exception as e:\n",
    "                # Opzionale: loggare errori di parsing\n",
    "                # print(f\"Errore nel processare la SMILES '{smi}': {e}\")\n",
    "                pass # Ignora SMILES che causano errori\n",
    "    print(f\"Processed {count_processed} rows, loaded {len(training_smiles_set)} valid canonical SMILES from training set.\")\n",
    "else:\n",
    "    print(f\"Warning: Training file '{training_file}' not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd0e1bd3-1168-48ae-8109-311b8d70c474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Esempio di esecuzione ---\n",
    "num_batches   = 300\n",
    "batch_size    = 1000\n",
    "temperature   = 0.90\n",
    "out_csv_path  = \"/home/grad/Desktop/pietro/denovo/Esercizio_prova/ge_1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d1f76a-19b5-4e4b-912f-eea40a59e36b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/300: generated 1000 SMILES\n",
      "Batch 2/300: generated 1000 SMILES\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem import QED, RDConfig\n",
    "sys.path.append(os.path.join(RDConfig.RDContribDir, 'SA_Score'))\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "import sascorer\n",
    "evaluate_and_save_batches(\n",
    "    model, char2idx, idx2char, max_length,\n",
    "    training_smiles_set, out_csv_path,\n",
    "    num_batches=num_batches,\n",
    "    batch_size=batch_size,\n",
    "    temperature=temperature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6713b7d4-63bc-49bb-b854-1b473084bcba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19766ef-53aa-41d8-943c-31b24e0793b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20db8cb2-1819-407a-ad34-59cd0250d5d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c89d17-7535-445b-907e-304fe56b4bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cec5f8-8fff-4e5f-8b43-2736fa5a1932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2936af47-5b8e-434b-8e92-3cb47f66ace3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
